# æœ€çµ‚èª²é¡Œ: ç¤¾ä¼šçš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ« ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

- **ãƒ†ãƒ¼ãƒ**: ã€Œä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã€ã‚’äºˆæ¸¬ã™ã‚‹ç¤¾ä¼šçš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«
- **ç’°å¢ƒ**: PettingZoo ã® simple_tag_v3
- **æœŸé–“**: 2026å¹´1æœˆ14æ—¥ ï½ 2026å¹´2æœˆä¸Šæ—¬ï¼ˆç´„3é€±é–“ï¼‰
- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: MATWM (Multi-Agent Transformer World Model)

---

## ã‚¿ã‚¹ã‚¯è©³ç´°: Simple Tag

### ç’°å¢ƒä»•æ§˜

- **è¦³æ¸¬ç©ºé–“**: 
  - adversary: 16æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ« (self_vel, self_pos, landmarks, other agents)
  - agent (good): 14æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ« (self_vel, self_pos, landmarks, other agents)
- **è¡Œå‹•ç©ºé–“**: 0ï½4ã®é›¢æ•£çš„ãªæ•´æ•°å€¤
  - `0`: no_action
  - `1`: move_left
  - `2`: move_right
  - `3`: move_down
  - `4`: move_up
- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°**: 4
  - 3 adversaries (red) - é…ã„ãŒã€goodã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ•ã¾ãˆã‚‹ã¨å ±é…¬+10
  - 1 good agent (green) - é€Ÿã„ãŒã€adversariesã«æ•ã¾ã‚‹ã¨å ±é…¬-10
- **æœ€å¤§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·**: 25ã‚¹ãƒ†ãƒƒãƒ— (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
- **ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹**: 62æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«

### è©•ä¾¡æŒ‡æ¨™

- å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç´¯ç©å ±é…¬
- å”èª¿/ç«¶äº‰ã®æˆåŠŸç‡ï¼ˆgood agentã®ç”Ÿå­˜ç‡ã€adversariesã®æ•ç²æˆåŠŸç‡ï¼‰
- ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ï¼ˆç›®æ¨™: 100Kç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—ä»¥å†…ã§è‰¯å¥½ãªæ€§èƒ½ï¼‰

---

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ&ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 

```
æœ€çµ‚èª²é¡Œ/
â”‚
â”œâ”€â”€ PROJECT_STRUCTURE.md                      # æœ¬ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ èª¬æ˜ï¼‰
â”œâ”€â”€ README.md                                 # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦ãƒ»é¸å®šç†ç”±
â”œâ”€â”€ simple_tag.md                             # Simple Tagç’°å¢ƒã®è©³ç´°ä»•æ§˜
â”‚
â”œâ”€â”€ 2026_SocialWorldModel_simple_tag_Baseline.ipynb  # â˜…ãƒ¡ã‚¤ãƒ³Notebookâ˜…ï¼ˆæ—§ç‰ˆï¼‰
â”œâ”€â”€ 2026_MATWM_simple_tag_Implementation.ipynb       # â˜…MATWMãƒ•ãƒ«å®Ÿè£…ç‰ˆâ˜…ï¼ˆæ–°è¦ä½œæˆï¼‰
â”‚   â”‚
â”‚   â””â”€â”€ ä¸»è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³:
â”‚       â”œâ”€â”€ 1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ»ç’°å¢ƒç¢ºèª
â”‚       â”œâ”€â”€ 2. ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆReplay Bufferï¼‰
â”‚       â”œâ”€â”€ 3. ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…
â”‚       â”‚   â”œâ”€â”€ Encoder/Decoder (Vector â†’ Categorical Latent)
â”‚       â”‚   â”œâ”€â”€ Dynamics Model (Transformer-based)
â”‚       â”‚   â”œâ”€â”€ Reward Predictor
â”‚       â”‚   â”œâ”€â”€ Continuation Predictor
â”‚       â”‚   â””â”€â”€ Teammate Predictor â˜…ç¤¾ä¼šçš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®ã‚³ã‚¢â˜…
â”‚       â”œâ”€â”€ 4. ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆPrioritized Replayï¼‰
â”‚       â”œâ”€â”€ 5. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…
â”‚       â”‚   â”œâ”€â”€ Actor Network
â”‚       â”‚   â”œâ”€â”€ Critic Network (Semi-centralized)
â”‚       â”‚   â””â”€â”€ Imagination-based Training
â”‚       â”œâ”€â”€ 6. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å­¦ç¿’
â”‚       â”œâ”€â”€ 7. è©•ä¾¡ãƒ»å¯è¦–åŒ–
â”‚       â””â”€â”€ 8. å‚è€ƒæ–‡çŒ®
â”‚
â”œâ”€â”€ data/                                      # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”‚   â”œâ”€â”€ replay_buffers/                        # Replay Bufferä¿å­˜å…ˆ
â”‚   â”‚   â”œâ”€â”€ adversary_0/
â”‚   â”‚   â”œâ”€â”€ adversary_1/
â”‚   â”‚   â”œâ”€â”€ adversary_2/
â”‚   â”‚   â””â”€â”€ agent_0/
â”‚   â””â”€â”€ episodes/                              # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜éŒ²
â”‚       â”œâ”€â”€ episode_0000.npz
â”‚       â”œâ”€â”€ episode_0001.npz
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ results/                                   # å­¦ç¿’çµæœãƒ»ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ
â”‚   â”œâ”€â”€ world_model/                           # ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
â”‚   â”‚   â””â”€â”€ YYYY_MM_DD_HH_MM_SS/
â”‚   â”‚       â”œâ”€â”€ checkpoints/
â”‚   â”‚       â”‚   â”œâ”€â”€ step_0000/
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ encoder_decoder.pt
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ dynamics.pt
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ reward_predictor.pt
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ continuation_predictor.pt
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ teammate_predictor.pt
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ optimizer.pt
â”‚   â”‚       â”‚   â”œâ”€â”€ step_10000/
â”‚   â”‚       â”‚   â””â”€â”€ ...
â”‚   â”‚       â””â”€â”€ visualizations/
â”‚   â”‚           â”œâ”€â”€ latent_reconstruction_step_0000.png
â”‚   â”‚           â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ agents/                                # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
â”‚       â””â”€â”€ YYYY_MM_DD_HH_MM_SS/
â”‚           â”œâ”€â”€ checkpoints/
â”‚           â”‚   â”œâ”€â”€ step_0000/
â”‚           â”‚   â”‚   â”œâ”€â”€ actor.pt
â”‚           â”‚   â”‚   â”œâ”€â”€ critic.pt
â”‚           â”‚   â”‚   â””â”€â”€ optimizer.pt
â”‚           â”‚   â”œâ”€â”€ step_10000/
â”‚           â”‚   â””â”€â”€ ...
â”‚           â””â”€â”€ logs/
â”‚               â””â”€â”€ training_metrics.csv
â”‚
â”œâ”€â”€ scripts/                                   # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ collect_data.py                        # ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ train_world_model.py                   # ä¸–ç•Œãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
â”‚   â”œâ”€â”€ train_agents.py                        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
â”‚   â””â”€â”€ evaluate.py                            # è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚
â”œâ”€â”€ è«–æ–‡/                                      # å‚è€ƒè«–æ–‡ãƒ»è³‡æ–™
â”‚   â”œâ”€â”€ md/
â”‚   â”‚   â”œâ”€â”€ TransformerWorldModelForSampleEfficientMultiAgentReinforcementLearning.md
â”‚   â”‚   â””â”€â”€ ActiveWorldModelLearningWithProgress.md
â”‚   â””â”€â”€ *.pdf
â”‚
â””â”€â”€ ä¸­é–“å ±å‘Š/                                  # ä¸­é–“å ±å‘Šè³‡æ–™
    â””â”€â”€ æå‡ºå†…å®¹.pdf
```

---

## ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

> ğŸ’¡ **è¨­è¨ˆæ–¹é‡**: å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ MATWM è«–æ–‡ (arXiv:2506.18537) ã®å®Ÿè¨¼çµæœã«åŸºã¥ãé¸å®šã—ã¦ã„ã¾ã™ã€‚

### 1. ä¸–ç•Œãƒ¢ãƒ‡ãƒ« (World Model)

#### Encoder/Decoder (Categorical VAE)
- **å½¹å‰²**: ãƒ™ã‚¯ãƒˆãƒ«è¦³æ¸¬ã‚’æ½œåœ¨ç©ºé–“ï¼ˆCategorical VAEï¼‰ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: MLP-based Encoder/Decoder
- **æ½œåœ¨è¡¨ç¾**: 32 categorical variables Ã— 32 classes = 1024æ¬¡å…ƒé›¢æ•£ç©ºé–“

**ğŸ“¥ å…¥åŠ› (Encoder)**:
- è¦³æ¸¬ãƒ™ã‚¯ãƒˆãƒ« `obs`: `[batch, obs_dim]`
  - adversary: `[batch, 16]` (self_vel[2] + self_pos[2] + landmarks[6] + other_agents[6])
  - good agent: `[batch, 14]` (self_vel[2] + self_pos[2] + landmarks[6] + other_agents[4])

**ğŸ“¤ å‡ºåŠ› (Encoder)**:
- æ½œåœ¨åˆ†å¸ƒ `logits`: `[batch, 32, 32]` (32å€‹ã® categorical å¤‰æ•°ã€å„32ã‚¯ãƒ©ã‚¹)
- ã‚µãƒ³ãƒ—ãƒ« `z`: `[batch, 32]` (32å€‹ã® one-hot â†’ 32å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)

**ğŸ“¥ å…¥åŠ› (Decoder)**:
- æ½œåœ¨çŠ¶æ…‹ `z`: `[batch, 32]` (categorical ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)
- ã¾ãŸã¯ `z_onehot`: `[batch, 32*32=1024]` (one-hot å±•é–‹)

**ğŸ“¤ å‡ºåŠ› (Decoder)**:
- å†æ§‹æˆè¦³æ¸¬ `obs_recon`: `[batch, obs_dim]` (å…ƒã®è¦³æ¸¬ç©ºé–“ã«å¾©å…ƒ)

- **ğŸ“š è«–æ–‡æ ¹æ‹ **: 
  - MATWM Section 2.1: "discrete latent spaces often outperform continuous ones"
  - Table 1: Categorical VAE ã‚’æ¡ç”¨ï¼ˆMAMBA ã¨åŒæ§˜ï¼‰
  - é€£ç¶šç©ºé–“ã‚ˆã‚Šæƒ…å ±åœ§ç¸®åŠ¹ç‡ãŒé«˜ãã€RL ã‚¿ã‚¹ã‚¯ã§å®Ÿè¨¼æ¸ˆã¿
- **Simple Tag ã¸ã®é©ç”¨**: ãƒ™ã‚¯ãƒˆãƒ«è¦³æ¸¬ï¼ˆ14/16æ¬¡å…ƒï¼‰ã‚’åŠ¹ç‡çš„ã«åœ§ç¸®ã—ã€äºˆæ¸¬æ€§èƒ½ã‚’å‘ä¸Š

#### Dynamics Model (Transformer)
- **å½¹å‰²**: éå»ã®æ½œåœ¨çŠ¶æ…‹ $z_t$ ã¨è¡Œå‹• $a_t$ ã‹ã‚‰æ¬¡ã®æ½œåœ¨çŠ¶æ…‹ $z_{t+1}$ ã‚’äºˆæ¸¬
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: Vanilla Transformer with Action Mixer (4å±¤ã€8ãƒ˜ãƒƒãƒ‰ã€512æ¬¡å…ƒ)

**ğŸ“¥ å…¥åŠ›**:
- æ½œåœ¨çŠ¶æ…‹ç³»åˆ— `z_seq`: `[batch, seq_len, latent_dim]`
  - `seq_len`: éå»ã®æ™‚ç³»åˆ—é•·ï¼ˆä¾‹: 64 stepsï¼‰
  - `latent_dim`: 32 (categorical ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹) or 1024 (one-hot)
- è¡Œå‹•ç³»åˆ— `action_seq`: `[batch, seq_len, 1]`
  - **Action Scaled**: agent ID ã«ã‚ˆã£ã¦ã‚ªãƒ•ã‚»ãƒƒãƒˆæ¸ˆã¿
    - Agent 0: 0-4
    - Agent 1: 5-9
    - Agent 2: 10-14
    - Agent 3: 15-19

**ğŸ”„ å†…éƒ¨å‡¦ç†**:
1. **Embedding**: `z_seq` ã¨ `action_seq` ã‚’åŸ‹ã‚è¾¼ã¿ â†’ `[batch, seq_len, 512]`
2. **Positional Encoding**: æ™‚ç³»åˆ—ä½ç½®æƒ…å ±ã‚’ä»˜åŠ 
3. **Transformer Layers** (Ã—4): 
   - Multi-Head Self-Attention (8 heads)
   - Feed-Forward Network
   - Layer Normalization & Residual Connection
4. **Output Projection**: `[batch, seq_len, 32*32]` (æ¬¡çŠ¶æ…‹ã® logits)

**ğŸ“¤ å‡ºåŠ›**:
- æ¬¡æ½œåœ¨çŠ¶æ…‹åˆ†å¸ƒ `logits_next`: `[batch, seq_len, 32, 32]`
  - å„æ™‚åˆ»ã®æ¬¡çŠ¶æ…‹ã‚’äºˆæ¸¬ï¼ˆ32 categorical Ã— 32 classesï¼‰
- ã‚µãƒ³ãƒ—ãƒ« `z_next`: `[batch, seq_len, 32]`

- **ğŸ“š è«–æ–‡æ ¹æ‹ **: 
  - MATWM Section 2: "transformers typically outperform RNNs due to long-range dependencies"
  - STORM ãƒ™ãƒ¼ã‚¹ã€RNN/GRU ã‚’ä½¿ã† MAMBA/MBVD ã‚’ä¸Šå›ã‚‹æ€§èƒ½
  - Table C.6: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã«æº–æ‹ 
- **åˆ©ç‚¹**: é•·æœŸä¾å­˜é–¢ä¿‚ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€ä¸¦åˆ—è¨ˆç®—å¯èƒ½

#### Reward Predictor (Two-hot Symlog)
- **å½¹å‰²**: æ½œåœ¨çŠ¶æ…‹ã‹ã‚‰å ±é…¬ã‚’äºˆæ¸¬
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: MLP (256æ¬¡å…ƒhiddenã€2å±¤)

**ğŸ“¥ å…¥åŠ›**:
- æ½œåœ¨çŠ¶æ…‹ `z`: `[batch, latent_dim]`
  - `latent_dim`: 32 (categorical) or 1024 (one-hot)
- ã¾ãŸã¯æ™‚ç³»åˆ—: `z_seq`: `[batch, seq_len, latent_dim]`

**ğŸ”„ å†…éƒ¨å‡¦ç†**:
1. **MLP**: `z` â†’ hidden(256) â†’ output(255)
2. **Symlog å¤‰æ›**: å ±é…¬ `r` ã‚’ symlog ç©ºé–“ã«å¤‰æ›
   - `symlog(r) = sign(r) * log(1 + |r|)`
   - ç¯„å›²: `[-20, +20]` â†’ 255 bins ã«é›¢æ•£åŒ–
3. **Two-hot Encoding**: 
   - 2ã¤ã®éš£æ¥ãƒ“ãƒ³ã«ç¢ºç‡ã‚’åˆ†é…
   - ä¾‹: symlog(r) = 3.7 â†’ bin[3] ã¨ bin[4] ã«é‡ã¿ä»˜ãåˆ†é…

**ğŸ“¤ å‡ºåŠ›**:
- å ±é…¬åˆ†å¸ƒ `logits`: `[batch, 255]` (255 bins over symlog space)
- äºˆæ¸¬å ±é…¬å€¤ `r_pred`: `[batch, 1]`
  - `r_pred = symexp(weighted_sum_over_bins)`
  - `symexp(x) = sign(x) * (exp(|x|) - 1)` (é€†å¤‰æ›)

- **ğŸ“š è«–æ–‡æ ¹æ‹ **: 
  - MATWM Section 3, Equation 5: symlog two-hot loss æ¡ç”¨
  - Dreamer V3 ã‹ã‚‰ç¶™æ‰¿ã—ãŸæ‰‹æ³•ã§ã€æ¥µç«¯ãªå ±é…¬å€¤ã«ãƒ­ãƒã‚¹ãƒˆ
- **Simple Tag ã¸ã®åŠ¹æœ**: Â±10ã®å ±é…¬ã‚’æ»‘ã‚‰ã‹ã«å­¦ç¿’å¯èƒ½ã€å¤–ã‚Œå€¤ã«å¼·ã„

#### Continuation Predictor
- **å½¹å‰²**: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ãƒ•ãƒ©ã‚°ã‚’äºˆæ¸¬ï¼ˆcontinues = 1 - doneï¼‰
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: MLP (256æ¬¡å…ƒhiddenã€2å±¤)

**ğŸ“¥ å…¥åŠ›**:
- æ½œåœ¨çŠ¶æ…‹ `z`: `[batch, latent_dim]`
  - `latent_dim`: 32 (categorical) or 1024 (one-hot)
- ã¾ãŸã¯æ™‚ç³»åˆ—: `z_seq`: `[batch, seq_len, latent_dim]`

**ğŸ”„ å†…éƒ¨å‡¦ç†**:
1. **MLP**: `z` â†’ hidden(256) â†’ output(1)
2. **Sigmoid**: logit â†’ ç¢ºç‡ `[0, 1]`

**ğŸ“¤ å‡ºåŠ›**:
- ç¶™ç¶šç¢ºç‡ `continues`: `[batch, 1]` or `[batch, seq_len, 1]`
  - `continues = 1`: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç¶™ç¶š
  - `continues = 0`: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº† (done)
- Bernoulli åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

- **ğŸ“š è«–æ–‡æ ¹æ‹ **: 
  - MATWM Equation 6: Binary cross-entropy loss
  - Imagination rollout ã®å“è³ªå‘ä¸Šã«ä¸å¯æ¬ 
- **Simple Tag ã§ã®é‡è¦æ€§**: 
  - Good agent ãŒæ•ã¾ã£ãŸã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’æ­£ç¢ºã«äºˆæ¸¬
  - æƒ³åƒè»Œé“ã§é©åˆ‡ã«çµ‚äº†åˆ¤å®š

#### Teammate Predictor â˜…ç¤¾ä¼šçš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®ã‚³ã‚¢â˜…
- **å½¹å‰²**: ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã‚’äºˆæ¸¬
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã«ç‹¬ç«‹ã—ãŸ MLP (256æ¬¡å…ƒhiddenã€2å±¤)

**ğŸ“¥ å…¥åŠ›**:
- **Focal agent** ã®æ½œåœ¨çŠ¶æ…‹ `z_focal`: `[batch, latent_dim]`
  - `latent_dim`: 32 (categorical) or 1024 (one-hot)
  - è‡ªèº«ã®è¦³æ¸¬ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæ½œåœ¨è¡¨ç¾
- ã¾ãŸã¯æ™‚ç³»åˆ—: `z_seq`: `[batch, seq_len, latent_dim]`

**ğŸ”„ å†…éƒ¨å‡¦ç†**:
1. **Agent-specific MLP** (å„ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨):
   - Agent 0 ã‚’äºˆæ¸¬: `MLP_0(z_focal)` â†’ logits `[5]`
   - Agent 1 ã‚’äºˆæ¸¬: `MLP_1(z_focal)` â†’ logits `[5]`
   - Agent 2 ã‚’äºˆæ¸¬: `MLP_2(z_focal)` â†’ logits `[5]`
   - (Simple Tag: 3ä½“ã®ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)
2. **Softmax**: logits â†’ è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ

**ğŸ“¤ å‡ºåŠ›**:
- ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•åˆ†å¸ƒï¼ˆå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç‹¬ç«‹ï¼‰:
  - Agent 0: `logits_0`: `[batch, 5]` â†’ `probs_0`: `[batch, 5]`
  - Agent 1: `logits_1`: `[batch, 5]` â†’ `probs_1`: `[batch, 5]`
  - Agent 2: `logits_2`: `[batch, 5]` â†’ `probs_2`: `[batch, 5]`
- **Unscaled action space**: 0-4 (original action space)
  - Dynamics Model ã¸ã®å…¥åŠ›æ™‚ã«å†åº¦ Action Scaling ã‚’é©ç”¨

**ğŸ”„ Imagination ã§ã®ä½¿ç”¨**:
```python
# Rollout æ™‚ã®ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
z_t = current_latent_state  # [1, latent_dim]
teammate_actions = []
for agent_id in other_agents:
    action_probs = teammate_predictor[agent_id](z_t)  # [1, 5]
    action = sample(action_probs)  # ç¢ºç‡çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    scaled_action = scale_action(action, agent_id)  # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    teammate_actions.append(scaled_action)

# æ¬¡çŠ¶æ…‹äºˆæ¸¬
z_next = dynamics_model(z_t, focal_action, teammate_actions)
```

- **ğŸ“š è«–æ–‡æ ¹æ‹ **: 
  - MATWM Section 3.1, Equation 8: teammate predictor ã®å®šç¾©
  - Abstract: "lightweight and effective teammate predictor module"
  - **Ablation Study (Table 5)**: Teammate Predictor ãªã—ã§ã¯æ€§èƒ½ãŒåŠ‡çš„ã«ä½ä¸‹
    - 8m: 67.0 â†’ 0.0 (å®Œå…¨å´©å£Š)
    - so_many_baneling: 74.0 â†’ 0.0 (å®Œå…¨å´©å£Š)
  - Section 4.3: å”èª¿ã‚¿ã‚¹ã‚¯ã§ "substantial gains"
- **åŠ¹æœ**: 
  - **éå®šå¸¸æ€§ã®è»½æ¸›**: ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ–¹ç­–å¤‰åŒ–ã‚’è¿½è·¡
  - **å”èª¿è¡Œå‹•**: Adversaries ãŒäº’ã„ã®å‹•ãã‚’äºˆæ¸¬ã—ã¦é€£æº
  - **ç«¶äº‰è¡Œå‹•**: Good agent ãŒ Adversaries ã®è¿½è·¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’äºˆæ¸¬ã—ã¦é€ƒèµ°
  - **Imagination rollout**: ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¦å­¦ç¿’
- **Simple Tag ã§ã®é‡è¦æ€§**: 
  - **Adversaries**: ä»–2ä½“ã®å‹•ãã‚’äºˆæ¸¬ã—ã¦åŒ…å›²æˆ¦è¡“ã‚’è¨ˆç”»
  - **Good Agent**: 3ä½“ã®è¿½è·¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’äºˆæ¸¬ã—ã¦æœ€é©é€ƒèµ°ãƒ«ãƒ¼ãƒˆé¸æŠ

### 2. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (Agent)

#### Actor Network
- **å½¹å‰²**: æ–¹ç­– $\pi(a|z)$ ã‚’å­¦ç¿’
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: MLP (256æ¬¡å…ƒhiddenã€2å±¤)

**ğŸ“¥ å…¥åŠ›**:
- æ½œåœ¨çŠ¶æ…‹ `z`: `[batch, latent_dim]`
  - å®Ÿç’°å¢ƒã¾ãŸã¯æƒ³åƒç’°å¢ƒã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæ½œåœ¨çŠ¶æ…‹
  - `latent_dim`: 32 (categorical) or 1024 (one-hot)
- ã¾ãŸã¯æƒ³åƒè»Œé“: `z_imagination`: `[batch, horizon, latent_dim]`
  - `horizon`: 15 (æƒ³åƒã™ã‚‹æœªæ¥ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°)

**ğŸ”„ å†…éƒ¨å‡¦ç†**:
1. **MLP**: `z` â†’ hidden(256) â†’ hidden(256) â†’ output(5)
2. **Softmax**: logits â†’ è¡Œå‹•ç¢ºç‡åˆ†å¸ƒ

**ğŸ“¤ å‡ºåŠ›**:
- è¡Œå‹•åˆ†å¸ƒ `logits`: `[batch, 5]` or `[batch, horizon, 5]`
  - 5ã¤ã®è¡Œå‹•ã‚¯ãƒ©ã‚¹: {0: no_action, 1: left, 2: right, 3: down, 4: up}
- è¡Œå‹•ç¢ºç‡ `probs`: `[batch, 5]`
- ã‚µãƒ³ãƒ—ãƒ«ã•ã‚ŒãŸè¡Œå‹• `action`: `[batch, 1]` (categorical sampling)
- ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ `entropy`: `[batch, 1]` (æ¢ç´¢ä¿ƒé€²ç”¨)

**æå¤±é–¢æ•°**:
```
L_actor = -ğ”¼[advantages * log_prob(action)] - Î² * entropy
```
- `advantages`: Critic ã‹ã‚‰è¨ˆç®—ã•ã‚ŒãŸåˆ©å¾—
- `Î²`: entropy coefficient (0.001)

- **ğŸ“š è«–æ–‡æ ¹æ‹ **: 
  - MATWM Equation 10, 11: Actor ã®å®šç¾©ã¨æå¤±é–¢æ•°
  - Entropy regularization ã§æ¢ç´¢ã‚’ä¿ƒé€²

#### Critic Network (Semi-centralized)
- **å½¹å‰²**: ä¾¡å€¤é–¢æ•° $V(z)$ ã‚’å­¦ç¿’
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: MLP (256æ¬¡å…ƒhiddenã€2å±¤)

**ğŸ“¥ å…¥åŠ›**:
- **Primary**: Focal agent ã®æ½œåœ¨çŠ¶æ…‹ `z`: `[batch, latent_dim]`
- **Optional (Semi-centralized)**: Teammate Predictor ã‹ã‚‰ã®ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•äºˆæ¸¬
  - è¨“ç·´æ™‚: æƒ³åƒä¸Šã®ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•æƒ…å ±ã‚’æš—é»™çš„ã«åˆ©ç”¨
  - å®Ÿè¡Œæ™‚: `z` ã®ã¿ã§ä¾¡å€¤æ¨å®šï¼ˆDecentralized Executionï¼‰

**ğŸ”„ å†…éƒ¨å‡¦ç†**:
1. **MLP**: `z` â†’ hidden(256) â†’ hidden(256) â†’ output(1)
2. ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å½±éŸ¿ã¯ `z` è‡ªä½“ã«å«ã¾ã‚Œã‚‹ï¼ˆè¦³æ¸¬ã«ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½ç½®ãŒå«ã¾ã‚Œã‚‹ãŸã‚ï¼‰

**ğŸ“¤ å‡ºåŠ›**:
- çŠ¶æ…‹ä¾¡å€¤ `V(z)`: `[batch, 1]` or `[batch, horizon, 1]`
  - ç¾åœ¨çŠ¶æ…‹ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹æœŸå¾…ç´¯ç©å ±é…¬
- æƒ³åƒè»Œé“ã®å ´åˆ: `V_imagination`: `[batch, horizon, 1]`

**æå¤±é–¢æ•°**:
```
L_critic = ğ”¼[(V(z) - target_value)Â²]
target_value = r + Î³ * continues * V(z_next)  (TD target)
ã¾ãŸã¯
target_value = Î»-return (GAE ã«ã‚ˆã‚‹ advantage è¨ˆç®—)
```

**GAE (Generalized Advantage Estimation)**:
```
A_t = Î£_{l=0}^{horizon} (Î³Î»)^l * Î´_{t+l}
Î´_t = r_t + Î³ * continues_t * V(z_{t+1}) - V(z_t)
```
- `Î³`: discount factor (0.99)
- `Î»`: GAE lambda (0.95)

- **ğŸ“š è«–æ–‡æ ¹æ‹ **: 
  - MATWM Table 1: "Semi-centralized" critic ã‚’æ¡ç”¨
  - Section 3.2: "semi-centralized critic that does not require having direct access to non-focal agent information"
  - Equation 12: Î»-return ã«ã‚ˆã‚‹ advantage è¨ˆç®—
  - ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®**æƒ³åƒä¸Šã®è¡Œå‹•**ã‚’è€ƒæ…®ï¼ˆç›´æ¥çš„ãªæƒ…å ±ã‚¢ã‚¯ã‚»ã‚¹ã¯ä¸è¦ï¼‰
- **åˆ©ç‚¹**: 
  - **Centralized (MAMBA)** ã®ã‚ˆã†ã«ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£å•é¡Œãªã—
  - **Decentralized** ã‚ˆã‚Šå”èª¿æ€§ãŒé«˜ã„
  - **CTDE** (Centralized Training, Decentralized Execution) ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã«æº–æ‹ 
  - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°ãŒå¢—ãˆã¦ã‚‚ç·šå½¢ã«ã‚¹ã‚±ãƒ¼ãƒ«

#### Imagination-based Training
- **ãƒ—ãƒ­ã‚»ã‚¹**: å®Ÿç’°å¢ƒã®çµŒé¨“ã‹ã‚‰æƒ³åƒè»Œé“ã‚’ç”Ÿæˆã—ã€Actor/Critic ã‚’å­¦ç¿’

**ğŸ“¥ å…¥åŠ›**:
- Replay Buffer ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã•ã‚ŒãŸå®Ÿä½“é¨“:
  - è¦³æ¸¬ `obs_real`: `[batch, obs_dim]`
  - è¡Œå‹• `action_real`: `[batch, 1]`
  - å ±é…¬ `reward_real`: `[batch, 1]`

**ğŸ”„ Imagination Rollout ãƒ—ãƒ­ã‚»ã‚¹**:

```python
# Step 1: å®Ÿè¦³æ¸¬ã‚’æ½œåœ¨çŠ¶æ…‹ã«å¤‰æ›
z_0 = encoder(obs_real)  # [batch, latent_dim]

# Step 2: æƒ³åƒè»Œé“ã‚’ç”Ÿæˆ (horizon=15 steps)
z_imagination = [z_0]
actions_imagination = []
rewards_imagination = []
continues_imagination = []

for t in range(horizon):  # t = 0, 1, ..., 14
    # 2.1: Actor ã§è¡Œå‹•ã‚’æ±ºå®š
    action_t = actor(z_t)  # [batch, 1], focal agent ã®è¡Œå‹•
    
    # 2.2: Teammate Predictor ã§ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•ã‚’äºˆæ¸¬
    teammate_actions_t = []
    for other_agent in other_agents:
        teammate_action = teammate_predictor[other_agent](z_t)
        teammate_actions_t.append(teammate_action)
    
    # 2.3: Dynamics Model ã§æ¬¡çŠ¶æ…‹ã‚’äºˆæ¸¬
    all_actions = [action_t] + teammate_actions_t  # [batch, n_agents]
    z_next = dynamics_model(z_t, all_actions)  # [batch, latent_dim]
    
    # 2.4: Reward Predictor ã§å ±é…¬ã‚’äºˆæ¸¬
    r_t = reward_predictor(z_next)  # [batch, 1]
    
    # 2.5: Continuation Predictor ã§çµ‚äº†åˆ¤å®š
    continues_t = continuation_predictor(z_next)  # [batch, 1]
    
    # è¨˜éŒ²
    z_imagination.append(z_next)
    actions_imagination.append(action_t)
    rewards_imagination.append(r_t)
    continues_imagination.append(continues_t)
    
    z_t = z_next  # æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã¸

# Step 3: æƒ³åƒè»Œé“ã‹ã‚‰ä¾¡å€¤ã¨Advantageã‚’è¨ˆç®—
V_imagination = critic(z_imagination)  # [batch, horizon+1, 1]
advantages = compute_gae(
    rewards_imagination,
    V_imagination,
    continues_imagination,
    Î³=0.99,
    Î»=0.95
)  # [batch, horizon, 1]
```

**ğŸ“¤ å‡ºåŠ› (å­¦ç¿’ã«ä½¿ç”¨)**:
- æƒ³åƒè»Œé“ã®æ½œåœ¨çŠ¶æ…‹: `z_imagination`: `[batch, horizon, latent_dim]`
- æƒ³åƒè»Œé“ã®è¡Œå‹•: `actions_imagination`: `[batch, horizon, 1]`
- æƒ³åƒè»Œé“ã®å ±é…¬: `rewards_imagination`: `[batch, horizon, 1]`
- æƒ³åƒè»Œé“ã®ç¶™ç¶šãƒ•ãƒ©ã‚°: `continues_imagination`: `[batch, horizon, 1]`
- æƒ³åƒè»Œé“ã®ä¾¡å€¤: `V_imagination`: `[batch, horizon, 1]`
- Advantages: `advantages`: `[batch, horizon, 1]`

**å­¦ç¿’æ›´æ–°**:
```python
# Actor æ›´æ–°
actor_loss = -ğ”¼[advantages * log_prob(actions_imagination)] - Î² * entropy

# Critic æ›´æ–°
critic_loss = ğ”¼[(V_imagination - target_values)Â²]
```

- **ğŸ“š è«–æ–‡æ ¹æ‹ **: 
  - MATWM Abstract: "imagine future trajectories" ã§å­¦ç¿’
  - Section 3.1: "agents learn entirely from imagination"
  - Table C.6: Imagination horizon = 16 (æˆ‘ã€…ã¯15ã‚’æ¡ç”¨)
  - Equation 12: Î»-return (GAE) ã«ã‚ˆã‚‹ advantage è¨ˆç®—
- **ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡**: 
  - å®Ÿç’°å¢ƒ**1ã‚¹ãƒ†ãƒƒãƒ—** â†’ æƒ³åƒ**15ã‚¹ãƒ†ãƒƒãƒ—** = **15å€ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿**
  - ç›®æ¨™: **50K-100K steps ã§åæŸ**ï¼ˆå¾“æ¥æ‰‹æ³•ã®1/10ä»¥ä¸‹ï¼‰
- **Simple Tag ã§ã®åŠ¹æœ**: 
  - å°‘ãªã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§å”èª¿ãƒ»ç«¶äº‰æˆ¦ç•¥ã‚’å­¦ç¿’
  - å±é™ºãªçŠ¶æ³ï¼ˆGood agent ãŒæ•ã¾ã‚‹ï¼‰ã‚’æƒ³åƒä¸Šã§å­¦ç¿’å¯èƒ½
  - Teammate Predictor ã«ã‚ˆã‚Šä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‹•ãã‚’è€ƒæ…®ã—ãŸè¨ˆç”»

### 3. è¨“ç·´æˆ¦ç•¥ (Training Strategy)

#### Prioritized Replay Buffer
- **å½¹å‰²**: æœ€è¿‘ã®çµŒé¨“ã‚’å„ªå…ˆçš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
- **å®Ÿè£…**: Exponential decay (0.995 per step)

**ğŸ“¥ å…¥åŠ› (çµŒé¨“ã®ä¿å­˜)**:
- è¦³æ¸¬ `obs`: `[obs_dim]` (14 or 16)
- è¡Œå‹• `action`: `[1]` (0-4, unscaled)
- å ±é…¬ `reward`: `[1]` (Â±10 or 0)
- æ¬¡è¦³æ¸¬ `next_obs`: `[obs_dim]`
- çµ‚äº†ãƒ•ãƒ©ã‚° `done`: `[1]` (0 or 1)
- ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— `t`: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã®æ™‚åˆ»

**ğŸ”„ å„ªå…ˆåº¦è¨ˆç®—**:
```python
# ä¿å­˜æ™‚ã«å„ªå…ˆåº¦ã‚’ä»˜ä¸
priority = decay_rate ** (current_step - t)
# decay_rate = 0.995
# current_step: ç¾åœ¨ã®å…¨ä½“ã‚¹ãƒ†ãƒƒãƒ—æ•°
# t: çµŒé¨“ãŒåé›†ã•ã‚ŒãŸæ™‚ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°

# ä¾‹:
# t=0 (å¤ã„çµŒé¨“), current_step=1000 â†’ priority = 0.995^1000 â‰ˆ 0.0067
# t=999 (æ–°ã—ã„çµŒé¨“), current_step=1000 â†’ priority = 0.995^1 â‰ˆ 0.995
```

**ğŸ“¤ å‡ºåŠ› (ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)**:
- ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒ« (ä¸–ç•Œãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨):
  - `obs_batch`: `[batch_size, seq_len, obs_dim]`
  - `action_batch`: `[batch_size, seq_len, 1]`
  - `reward_batch`: `[batch_size, seq_len, 1]`
  - `continues_batch`: `[batch_size, seq_len, 1]`
  - `batch_size`: 16
  - `seq_len`: 64 (æ™‚ç³»åˆ—ã®é•·ã•)

- ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒ« (ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ç”¨):
  - `obs_batch`: `[batch_size, obs_dim]` (å˜ä¸€ã‚¹ãƒ†ãƒƒãƒ—)
  - `batch_size`: 16
  - Imagination ã®é–‹å§‹ç‚¹ã¨ã—ã¦ä½¿ç”¨

**ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç¢ºç‡**:
```python
p_i = priority_i / Î£_j priority_j
# æœ€è¿‘ã®çµŒé¨“ã»ã©é«˜ç¢ºç‡ã§ã‚µãƒ³ãƒ—ãƒ«ã•ã‚Œã‚‹
```

- **ğŸ“š è«–æ–‡æ ¹æ‹ **: 
  - MATWM Section 3: "prioritized replay mechanism that trains the world model on recent experiences"
  - Section 3.2: éå®šå¸¸æ€§ï¼ˆä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ–¹ç­–å¤‰åŒ–ï¼‰ã¸ã®å¯¾å¿œ
  - Table C.6: Replay sampling priority decay = 0.9998 (æˆ‘ã€…ã¯0.995ã‚’æ¡ç”¨)
  - **Ablation Study (Table 5)**: PER ãªã—ã§ã¯æ€§èƒ½ä½ä¸‹
    - 8m: 65.0 â†’ 52.0
    - Pistonball: 92.6 â†’ 85.1
- **åŠ¹æœ**: 
  - å¤ã„çµŒé¨“ï¼ˆoutdated behaviorsï¼‰ã®å½±éŸ¿ã‚’è»½æ¸›
  - æ–¹ç­–ãŒé€²åŒ–ã—ã¦ã‚‚ world model ãŒè¿½å¾“
  - å­¦ç¿’ã®å®‰å®šæ€§å‘ä¸Š
  - éå®šå¸¸æ€§ï¼ˆä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ–¹ç­–å¤‰åŒ–ï¼‰ã¸ã®é©å¿œ

#### Action Scaling
- **å½¹å‰²**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã«è¡Œå‹•ç©ºé–“ã‚’ã‚ªãƒ•ã‚»ãƒƒãƒˆã—ã¦ã€World Model ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è­˜åˆ¥

**ğŸ“¥ å…¥åŠ›**:
- Original action `a`: `[1]` (0-4)
  - 0: no_action
  - 1: move_left
  - 2: move_right
  - 3: move_down
  - 4: move_up
- Agent ID `agent_id`: `[1]` (0, 1, 2, 3)

**ğŸ”„ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‡¦ç†**:
```python
def scale_action(action, agent_id, action_space_size=5):
    """
    è¡Œå‹•ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆIDãƒ™ãƒ¼ã‚¹ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¿½åŠ 
    """
    scaled_action = action + agent_id * action_space_size
    return scaled_action

def unscale_action(scaled_action, agent_id, action_space_size=5):
    """
    ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚ŒãŸè¡Œå‹•ã‚’å…ƒã«æˆ»ã™
    """
    action = scaled_action - agent_id * action_space_size
    return action
```

**ğŸ“¤ å‡ºåŠ›**:
- Scaled action `a_scaled`:
  - **Agent 0**: 0-4 (å¤‰åŒ–ãªã—)
  - **Agent 1**: 5-9
  - **Agent 2**: 10-14
  - **Agent 3**: 15-19

**ä½¿ç”¨ãƒ•ãƒ­ãƒ¼**:
```python
# 1. ç’°å¢ƒã‹ã‚‰è¡Œå‹•åé›†æ™‚
action_env = agent.select_action(obs)  # 0-4
action_scaled = scale_action(action_env, agent_id)  # 0-19

# 2. World Model å­¦ç¿’æ™‚
dynamics_input = [z_t, action_scaled]  # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¸ˆã¿è¡Œå‹•ã‚’å…¥åŠ›

# 3. Teammate Predictor å‡ºåŠ›æ™‚
teammate_action = teammate_predictor(z_t)  # 0-4 (unscaled)
teammate_action_scaled = scale_action(teammate_action, teammate_id)

# 4. Imagination rollout æ™‚
all_actions_scaled = [
    scale_action(focal_action, focal_id),
    scale_action(teammate_action_0, teammate_0_id),
    scale_action(teammate_action_1, teammate_1_id),
    scale_action(teammate_action_2, teammate_2_id),
]
z_next = dynamics_model(z_t, all_actions_scaled)
```

**è¡Œå‹•ç©ºé–“ã®æ‹¡å¼µ**:
- Original: 5 actions per agent
- Scaled: 20 actions (5 Ã— 4 agents)
- Dynamics Model ã®å‡ºåŠ›å±¤: 20æ¬¡å…ƒ

- **ğŸ“š è«–æ–‡æ ¹æ‹ **: 
  - MATWM Section 3: "action scaling mechanism to encode agent-specific information"
  - World Model ãŒ explicit ID ã‚„ embedding ãªã—ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è­˜åˆ¥å¯èƒ½
  - **Ablation Study (Table 5)**: Action Scaling ãªã—ã§ã¯æ€§èƒ½ä½ä¸‹ï¼ˆç‰¹ã«ç”»åƒãƒ™ãƒ¼ã‚¹ç’°å¢ƒï¼‰
    - Pistonball: 92.6 â†’ 88.4
    - Externality Mushrooms: 146.8 â†’ 135.7
- **åˆ©ç‚¹**: 
  - **ã‚·ãƒ³ãƒ—ãƒ«**: è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸è¦
  - **åŠ¹ç‡çš„**: è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—åŠ ãªã—
  - **è­˜åˆ¥å¯èƒ½**: Shared world model ã§ã‚‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
  - **Dynamics å­¦ç¿’**: ã€ŒAgent 1 ãŒå³ã«ç§»å‹•ã€ã¨ã€ŒAgent 2 ãŒå³ã«ç§»å‹•ã€ã‚’åŒºåˆ¥å¯èƒ½

#### Decentralized Execution
- **æ–¹é‡**: å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è‡ªèº«ã®è¦³æ¸¬ã®ã¿ã§è¡Œå‹•æ±ºå®š

**ğŸ“ è¨“ç·´æ™‚ (Centralized Training)**:

**ğŸ“¥ å…¥åŠ›**:
- å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çµŒé¨“ãŒ Replay Buffer ã«è“„ç©
- Teammate Predictor ãŒä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã‚’å­¦ç¿’
- Imagination rollout ã§å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç›¸äº’ä½œç”¨ã‚’è€ƒæ…®

**ğŸ”„ å‡¦ç†**:
```python
# å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã«ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
for agent in agents:
    # è‡ªèº«ã®è¦³æ¸¬ã‹ã‚‰æ½œåœ¨çŠ¶æ…‹
    z = encoder(agent.obs)
    
    # Teammate Predictor ã§ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆäºˆæ¸¬
    teammate_actions = [
        teammate_predictor[other_id](z)
        for other_id in other_agents
    ]
    
    # å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è€ƒæ…®ã—ãŸæ¬¡çŠ¶æ…‹äºˆæ¸¬
    z_next = dynamics_model(z, agent.action, teammate_actions)
    
    # Actor/Critic æ›´æ–°ï¼ˆä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæƒ…å ±ã‚’æš—é»™çš„ã«åˆ©ç”¨ï¼‰
    actor.update(z)
    critic.update(z)  # Semi-centralized
```

**ğŸ® å®Ÿè¡Œæ™‚ (Decentralized Execution)**:

**ğŸ“¥ å…¥åŠ›**:
- è‡ªèº«ã®è¦³æ¸¬ã®ã¿: `obs_focal`: `[obs_dim]`
- é€šä¿¡ä¸è¦ï¼ˆä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çŠ¶æ…‹ãƒ»è¡Œå‹•ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„ï¼‰

**ğŸ“¤ å‡ºåŠ›**:
- è¡Œå‹• `action`: `[1]` (0-4)

**ğŸ”„ å‡¦ç†**:
```python
# å®Ÿè¡Œæ™‚ã¯å®Œå…¨ã«ç‹¬ç«‹
z = encoder(obs_focal)  # è‡ªèº«ã®è¦³æ¸¬ã®ã¿
action = actor(z)  # è‡ªèº«ã®æ–¹ç­–ã®ã¿

# Teammate Predictor ã¯ä½¿ç”¨ã—ãªã„ï¼ˆå®Ÿè¡Œæ™‚ï¼‰
# ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å½±éŸ¿ã¯ obs ã«å«ã¾ã‚Œã‚‹ä½ç½®æƒ…å ±ã§é–“æ¥çš„ã«æŠŠæ¡
```

**ç‰¹å¾´**:
- âœ… **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•° N ã«å¯¾ã—ã¦ O(N) ã®è¨ˆç®—é‡
- âœ… **éƒ¨åˆ†è¦³æ¸¬ã«é©åˆ**: å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è‡ªèº«ã®è¦³æ¸¬ã®ã¿ã§è¡Œå‹•
- âœ… **é€šä¿¡ä¸è¦**: åˆ†æ•£å®Ÿè¡Œå¯èƒ½
- âœ… **ãƒ­ãƒã‚¹ãƒˆ**: ä¸€éƒ¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ•…éšœã«å¼·ã„

- **ğŸ“š è«–æ–‡æ ¹æ‹ **: 
  - MATWM Section 3.2: Decentralized world model approach
  - CTDE (Centralized Training, Decentralized Execution) è¨­è¨ˆ
  - Scalability: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°ã«å¯¾ã—ã¦ç·šå½¢ã«ã‚¹ã‚±ãƒ¼ãƒ«
- **Simple Tag ã§ã®é©ç”¨**: 
  - **è¨“ç·´**: Teammate Predictor ã§ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è€ƒæ…®ã€Semi-centralized Critic
  - **å®Ÿè¡Œ**: å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç‹¬ç«‹ã«è¡Œå‹•é¸æŠã€éƒ¨åˆ†è¦³æ¸¬ã®ã¿ä½¿ç”¨

---

## å®Ÿè¡Œãƒ•ãƒ­ãƒ¼

### 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
pip install torch numpy matplotlib tqdm h5py
pip install pettingzoo[mpe] supersuit
```

### 2. ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆWarm-upï¼‰

```python
# Notebookã¾ãŸã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å®Ÿè¡Œ
# - ãƒ©ãƒ³ãƒ€ãƒ æ–¹ç­–ã§1000ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã®çµŒé¨“ã‚’åé›†
# - Replay Bufferã«ä¿å­˜
```

### 3. ä¸–ç•Œãƒ¢ãƒ‡ãƒ«å­¦ç¿’

```python
# ä¸»ãªã‚¹ãƒ†ãƒƒãƒ—:
# 1. Replay Bufferã‹ã‚‰64é•·ç³»åˆ—ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆPrioritized Replayï¼‰
# 2. Encoder/Decoder, Dynamics, Reward, Continuation, Teammate Predictorã‚’æ›´æ–°
# 3. 1ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ï¼ˆå®Ÿç’°å¢ƒ1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ï¼‰
```

### 4. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’

```python
# ä¸»ãªã‚¹ãƒ†ãƒƒãƒ—:
# 1. Replay Bufferã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
# 2. ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã§15ã‚¹ãƒ†ãƒƒãƒ—ã®æƒ³åƒè»Œé“ã‚’ç”Ÿæˆ
# 3. Actor/Criticã‚’æƒ³åƒè»Œé“ã§æ›´æ–°
# 4. 1ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ï¼ˆå®Ÿç’°å¢ƒ1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ï¼‰
```

### 5. è©•ä¾¡ãƒ»å¯è¦–åŒ–

```python
# - ç´¯ç©å ±é…¬ã®æ¨ç§»
# - å­¦ç¿’æ›²ç·š
# - Teammate Predictionã®ç²¾åº¦
# - æ½œåœ¨ç©ºé–“ã®å¯è¦–åŒ–
```

---

## ç·¨é›†å¯èƒ½ãƒ»ä¸å¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³

### âœ… ç·¨é›†å¯èƒ½ï¼ˆè‡ªç”±ã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰

1. **ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**
   - æ½œåœ¨æ¬¡å…ƒæ•°ã€Transformerã®å±¤æ•°ãƒ»ãƒ˜ãƒƒãƒ‰æ•°
   - Teammate Predictorã®è¨­è¨ˆ
   - Actor/Criticã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ 

2. **å­¦ç¿’ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**
   - å­¦ç¿’ç‡ã€ãƒãƒƒãƒã‚µã‚¤ã‚º
   - Imagination horizon
   - Prioritized Replayã®æ¸›è¡°ç‡

3. **æ‹¡å¼µæ©Ÿèƒ½**
   - Î³-progresså¥½å¥‡å¿ƒã®å°å…¥ï¼ˆActive World Model Learningï¼‰
   - Communication moduleã®è¿½åŠ 
   - Attention mechanismã®å¼·åŒ–

### âŒ ç·¨é›†ä¸å¯ï¼ˆåŸºæœ¬çš„ã«ç¶­æŒï¼‰

1. **ç’°å¢ƒã®è¨­å®š**
   - simple_tag_v3ã®åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
   - è¦³æ¸¬ãƒ»è¡Œå‹•ç©ºé–“ã®å®šç¾©

2. **è©•ä¾¡æŒ‡æ¨™**
   - ç´¯ç©å ±é…¬ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡

---

## ç’°å¢ƒè¦ä»¶

- **Python**: 3.9+
- **ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**:
  - PyTorch 2.0+
  - PettingZoo[mpe]
  - NumPy
  - Matplotlib
  - tqdm
  - h5py (ã‚ªãƒ—ã‚·ãƒ§ãƒ³: ãƒ‡ãƒ¼ã‚¿ä¿å­˜)

---

## æ€§èƒ½å‘ä¸Šã®ã‚¢ã‚¤ãƒ‡ã‚¢

### ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®æ”¹å–„

- [ ] **Teammate Predictor ã®ç²¾åº¦å‘ä¸Š**
  - Attention æ©Ÿæ§‹ã®å°å…¥
  - è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—å…ˆã®è¡Œå‹•äºˆæ¸¬
  - ğŸ“š å‚è€ƒ: MATWM Section 4.3 ã§ç¤ºå”†ã•ã‚Œã¦ã„ã‚‹æ”¹å–„æ–¹å‘

- [ ] **Theory of Mind è¦ç´ ã®çµ„ã¿è¾¼ã¿**
  - ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ„å›³ãƒ»ä¿¡å¿µã®æ¨å®š
  - Recursive reasoning (ã€Œç›¸æ‰‹ã¯è‡ªåˆ†ã‚’ã©ã†äºˆæ¸¬ã—ã¦ã„ã‚‹ã‹ã€)
  - ğŸ“š å‚è€ƒ: AWML Section 7 ã® Theory of Mind discussion

- [ ] **Communication Module ã®è¿½åŠ **
  - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ passing æ©Ÿæ§‹
  - Attention-based communication
  - Differentiable communication

### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ”¹å–„

- [ ] **Î³-Progress Curiosity ã®å°å…¥** â­ æ¬¡ã®é‡è¦æ‹¡å¼µ
  - Active World Model Learning (AWML è«–æ–‡) ã®æ‰‹æ³•
  - ğŸ“š è«–æ–‡æ ¹æ‹ : AWML Equation 10, 11, 12
  - **åŠ¹æœ** (AWML Table 1):
    - Mixture World: 7.83å€ã®æ€§èƒ½å‘ä¸Š (vs Random)
    - Noise World: 13.79å€ã®æ€§èƒ½å‘ä¸Š
    - White Noise Problem ã®è§£æ±º
  - **å®Ÿè£…**:
    ```python
    # Î¸_old = (1-Î³) Î£ Î³^(k-1-i) Î¸_i  (exponential mixture)
    # Î¸_old â† Î³ Î¸_old + (1-Î³) Î¸_new
    # r = L(Î¸_old, x, a) - L(Î¸_new, x, a)
    ```
  - **Simple Tag ã§ã®æœŸå¾…åŠ¹æœ**:
    - å­¦ç¿’å¯èƒ½ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå”èª¿è¿½è·¡ã€åŠ¹ç‡çš„é€ƒèµ°ï¼‰ã«æ³¨åŠ›
    - ãƒã‚¤ã‚ºçš„ãªè¡Œå‹•ã‚’ç„¡è¦–
    - æ¢ç´¢åŠ¹ç‡ã®å¤§å¹…å‘ä¸Š

- [ ] **éšå±¤çš„ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°**
  - High-level goal selection
  - Low-level action execution
  - Feudal RL ã¨ã®çµ±åˆ

- [ ] **ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸ Critic ã®è¨­è¨ˆ**
  - Value decomposition
  - Distributional RL

- [ ] **Adversarial Training**
  - Self-play ã«ã‚ˆã‚‹ç¶™ç¶šå­¦ç¿’
  - Population-based training

### ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã®æ”¹å–„

- [ ] **Prioritized Replay ã®é‡ã¿ä»˜ã‘æœ€é©åŒ–**
  - ç¾åœ¨: Exponential decay (0.995)
  - æ¤œè¨: äºˆæ¸¬èª¤å·®ãƒ™ãƒ¼ã‚¹ã®å„ªå…ˆåº¦
  - ğŸ“š å‚è€ƒ: MATWM Table C.6 ã§ã¯ 0.9998 ã‚’ä½¿ç”¨

- [ ] **æ¨¡å€£å­¦ç¿’ã¨ã®çµ„ã¿åˆã‚ã›**
  - Expert demonstrations ã‹ã‚‰ã®å­¦ç¿’
  - Behavior cloning + RL

- [ ] **Self-play ã«ã‚ˆã‚‹ç¶™ç¶šå­¦ç¿’**
  - Good agent ã¨ Adversaries ã®ç›¸äº’é€²åŒ–
  - Curriculum learning

---

## å‚è€ƒæ–‡çŒ®

### ä¸»è¦è«–æ–‡

1. **MATWM (Multi-Agent Transformer World Model)** ğŸŒŸ
   - Deihim, A., Alonso, E., & Apostolopoulou, D. (2025)
   - "Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning"
   - arXiv:2506.18537v1 [cs.LG], 23 Jun 2025
   - **æœ¬å®Ÿè£…ã®ãƒ™ãƒ¼ã‚¹**: å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒã“ã®è«–æ–‡ã«åŸºã¥ã
   - **ä¸»è¦è²¢çŒ®**:
     - Teammate Predictor ã«ã‚ˆã‚‹ç¤¾ä¼šçš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«
     - 50K steps ã§æœ€å…ˆç«¯æ€§èƒ½ï¼ˆSMAC, PettingZoo, Melting Potï¼‰
     - åˆã®ç”»åƒãƒ™ãƒ¼ã‚¹å¯¾å¿œãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¸–ç•Œãƒ¢ãƒ‡ãƒ«

2. **Active World Model Learning with Progress Curiosity** ğŸ¯
   - Kim, K., Sano, M., De Freitas, J., Haber, N., & Yamins, D. (2020)
   - arXiv:2007.07853v1 [cs.LG], 15 Jul 2020
   - ICML 2020
   - **ä»Šå¾Œã®æ‹¡å¼µã¨ã—ã¦å°å…¥äºˆå®š**: Î³-Progress Curiosity
   - **ä¸»è¦è²¢çŒ®**:
     - Î³-Progress: å­¦ç¿’å¯èƒ½ãª dynamics ã«æ³¨æ„ã‚’å‘ã‘ã‚‹
     - White Noise Problem ã®è§£æ±º
     - Animate attention ã®è‡ªç„¶ãªç²å¾—
   - **æœŸå¾…åŠ¹æœ**: æ¢ç´¢åŠ¹ç‡ã®å¤§å¹…å‘ä¸Šï¼ˆTable 1: 7.83å€ in Mixture Worldï¼‰

### åŸºç›¤æŠ€è¡“

3. **STORM (Stochastic Transformer World Model)**
   - Zhang, W., et al. (2023)
   - NeurIPS 2023
   - **MATWM ã®åŸºç›¤**: å˜ä¸€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®æœ€å…ˆç«¯
   - Two-hot symlog, KL balance, free bits ãªã©ã‚’çµ±åˆ

4. **Dreamer V3**
   - Hafner, D., et al. (2023)
   - "Mastering Diverse Domains through World Models"
   - **è²¢çŒ®**: Two-hot symlog rewards, percentile return normalization
   - MATWM ãŒå¤šæ•°ã®æŠ€è¡“ã‚’ç¶™æ‰¿

### é–¢é€£æ‰‹æ³•

5. **MAMBA (Multi-Agent Model-Based RL)**
   - Egorov, V., & Shpilman, A. (2022)
   - Centralized world model approach
   - Categorical VAE æ¡ç”¨

6. **MARIE (Decentralized Transformers with Centralized Aggregation)**
   - Zhang, Y., et al. (2024)
   - arXiv:2406.15836
   - åˆã® Transformer ãƒ™ãƒ¼ã‚¹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¸–ç•Œãƒ¢ãƒ‡ãƒ«
   - Perceiver ã«ã‚ˆã‚‹ feature aggregation

7. **PettingZoo**
   - Terry, J. K., et al. (2021)
   - "PettingZoo: Gym for Multi-Agent Reinforcement Learning"
   - Simple Tag ç’°å¢ƒã‚’æä¾›

---

## MATWM ã¨ AWML ã®èåˆæˆ¦ç•¥

### ç¾çŠ¶: MATWM ã®ãƒ•ãƒ«å®Ÿè£… âœ…

**å®Ÿè£…æ¸ˆã¿ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**:
- âœ… Categorical VAE (Encoder/Decoder)
- âœ… Transformer Dynamics Model
- âœ… Two-hot Symlog Reward Predictor
- âœ… Continuation Predictor
- âœ… **Teammate Predictor** (ç¤¾ä¼šçš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®æ ¸å¿ƒ)
- âœ… Prioritized Replay Buffer
- âœ… Action Scaling
- âœ… Semi-centralized Critic
- âœ… Imagination-based Training

### æ¬¡æ®µéš: AWML Î³-Progress ã®çµ±åˆ ğŸ¯

#### çµ±åˆæ–¹æ³•

**1. Curiosity Reward ã®å®šç¾©**:
```python
# AWML Equation 10, 11, 12 ã«åŸºã¥ã
Î¸_old = exponential_mixture_of_past_models(Î³=0.9)
L_old = world_model_loss(Î¸_old, experience)
L_new = world_model_loss(Î¸_current, experience)
r_curiosity = L_old - L_new  # Progress = äºˆæ¸¬èª¤å·®ã®æ”¹å–„
```

**2. å ±é…¬ã®çµ±åˆ**:
```python
r_total = r_extrinsic + Î»_curiosity * r_curiosity
# r_extrinsic: Simple Tag ã®å ±é…¬ (Â±10)
# Î»_curiosity: å¥½å¥‡å¿ƒã®é‡ã¿ (0.1-1.0)
```

**3. å®Ÿè£…ä¸Šã®æ³¨æ„**:
- Teammate Predictor ã®äºˆæ¸¬èª¤å·®ã‚‚ Progress ã«å«ã‚ã‚‹
- Î³: 0.9-0.95 ãŒ AWML ã§æ¨å¥¨
- Î»_curiosity: æ¢ç´¢ã¨åˆ©ç”¨ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´

#### æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

- **æ¢ç´¢åŠ¹ç‡ã®å‘ä¸Š**: å­¦ç¿’å¯èƒ½ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå”èª¿è¿½è·¡ã€åŠ¹ç‡çš„é€ƒèµ°ï¼‰ã«æ³¨åŠ›
- **White Noise Problem ã®å›é¿**: ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œå‹•ã‚’ç„¡è¦–
- **ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡**: 50K steps â†’ 30K steps ä»¥ä¸‹ã¸ã®çŸ­ç¸®ã‚’æœŸå¾…
- **Teammate Prediction ã¨ã®ç›¸ä¹—åŠ¹æœ**: 
  - ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å­¦ç¿’å¯èƒ½ãªæ–¹ç­–ã«æ³¨æ„ã‚’å‘ã‘ã‚‹
  - äºˆæ¸¬ä¸å¯èƒ½ãªãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•ã‚’ç„¡è¦–

#### å®Ÿè£…å„ªå…ˆåº¦

1. **Phase 1 (ç¾åœ¨)**: MATWM ã®ã¿ã§ Simple Tag ã‚’å­¦ç¿’ âœ…
2. **Phase 2 (æ¬¡)**: Î³-Progress ã®çµ±åˆã¨ ablation study
3. **Phase 3**: Theory of Mind è¦ç´ ã®è¿½åŠ 

---

## è«–æ–‡ã¨ã®å¯¾å¿œè¡¨

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | MATWM è«–æ–‡ | AWML è«–æ–‡ | å®Ÿè£…çŠ¶æ³ |
|--------------|-----------|-----------|---------|
| Categorical VAE | Section 2.1, Table 1 | - | âœ… |
| Transformer Dynamics | Section 2 | - | âœ… |
| Reward Predictor | Equation 5 | - | âœ… |
| Continuation | Equation 6 | - | âœ… |
| Teammate Predictor | Section 3.1, Eq 8, Table 5 | - | âœ… |
| Prioritized Replay | Section 3, 3.2, Table 5 | - | âœ… |
| Action Scaling | Section 3, Table 5 | - | âœ… |
| Semi-centralized Critic | Section 3.2, Table 1 | - | âœ… |
| Imagination Training | Abstract, Section 3.1, Eq 12 | - | âœ… |
| Î³-Progress Curiosity | - | Eq 10-12, Table 1 | ğŸ”œ Phase 2 |
| Theory of Mind | Section 4.3 (discussion) | Section 7 | ğŸ”œ Phase 3 |

---

## æ›´æ–°å±¥æ­´

- 2026-01-14 (åˆç‰ˆ): ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ åˆç‰ˆä½œæˆ
- 2026-01-14 (ç¬¬2ç‰ˆ): MATWM ãƒ•ãƒ«å®Ÿè£…å®Œäº†ã€è«–æ–‡æ ¹æ‹ ã‚’å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«è¿½åŠ 
- 2026-01-14 (ç¬¬3ç‰ˆ): AWML Î³-Progress çµ±åˆæˆ¦ç•¥ã‚’è¿½åŠ ã€è«–æ–‡å¯¾å¿œè¡¨ã‚’æ•´å‚™

---

**Good Luck! ğŸƒâ€â™‚ï¸ğŸƒâ€â™€ï¸ğŸ¯**

