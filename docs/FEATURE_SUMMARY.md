# MATWMå®Ÿè£…å®Œäº†ã¾ã¨ã‚

## âœ… è¿½åŠ ã•ã‚ŒãŸ5ã¤ã®æ©Ÿèƒ½

### 1. **é‡ã¿ã®åˆæœŸåŒ–** âœ…
- `initialize_matwm_weights()`: Xavier/KaimingåˆæœŸåŒ–
- World Model, Actor, Criticå…¨ã¦ã«å¯¾å¿œ
- è«–æ–‡ã®æ¨å¥¨æ‰‹æ³•ã«åŸºã¥ã

### 2. **ä¿å­˜æ©Ÿèƒ½ã®å¼·åŒ–** âœ…
- `save_full_checkpoint()`: å®Œå…¨ãªè¨“ç·´çŠ¶æ…‹ã®ä¿å­˜
- `load_full_checkpoint()`: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å¾©å…ƒ
- å…±æœ‰World Model, å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ, ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚€

### 3. **çµŒéã®å¯è¦–åŒ–** âœ…
- `plot_training_progress()`: 9ãƒ‘ãƒãƒ«ã®è©³ç´°å¯è¦–åŒ–
  - Episode Rewards (ç§»å‹•å¹³å‡)
  - World Model Total Loss
  - Teammate Prediction Loss â˜…
  - WMã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ¥Loss
  - Actor/Critic Loss (ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥)
  - Imagined Reward/Value
  - Cumulative Rewards

### 4. **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç¢ºèª** âœ…
- `inspect_matwm_architecture()`: ç·åˆçš„ãªãƒ¢ãƒ‡ãƒ«æ¤œè¨¼
  - **4-1 ãƒ€ãƒŸãƒ¼å…¥åŠ›ãƒ†ã‚¹ãƒˆ**: å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‹•ä½œç¢ºèª
  - **4-2 å±¤æ•°ã‚«ã‚¦ãƒ³ãƒˆ**: `count_layers()`ã§Linear/Convå±¤ã‚’é›†è¨ˆ
  - **4-3 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ç¢ºèª**: `count_parameters()`ã§è©³ç´°åˆ†æ
  - **4-4 torchinfo summary**: è©³ç´°ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¡¨ç¤º

### 5. **è¨ˆç®—ç’°å¢ƒã®æŠŠæ¡** âœ…
- `print_gpu_info()`: GPUç’°å¢ƒã®å®Œå…¨è¡¨ç¤º
  - **å…¨GPUãƒªã‚¹ãƒˆ**: åˆ©ç”¨å¯èƒ½ãªå…¨GPU (A100, L4ç­‰)
  - **ç¾åœ¨ä½¿ç”¨ä¸­ã®GPU**: ãƒ‡ãƒã‚¤ã‚¹åã¨ID
  - **ãƒ¡ãƒ¢ãƒªæƒ…å ±**: Total/Allocated/Reserved
  - **GPUç¨®é¡åˆ¤å®š**: A100, V100, RTXç­‰ã‚’è‡ªå‹•è­˜åˆ¥

---

## ğŸ“ ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ (Rollout) ã«ã¤ã„ã¦

### å®šç¾©
**ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ = World Modelã‚’ä½¿ã£ãŸæƒ³åƒä¸Šã®æœªæ¥å±•é–‹**

```python
# ç¾åœ¨ã®çŠ¶æ…‹ã‹ã‚‰12ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§æƒ³åƒ
for t in range(imagination_horizon):  # â† 12å› (simple_tag)
    action = actor(z_current)
    z_next = world_model.predict_next(z_current, action)
    reward = world_model.predict_reward(z_next)
    # ... 12ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã®æœªæ¥ã‚’å±•é–‹
```

### å›æ•°ã¯æ±ºã¾ã£ã¦ã„ã‚‹
- **Imagination Horizon = 12** (simple_tag, 4ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)
- **Agent Batch Size = 768** (ä¸¦è¡Œä¸–ç•Œã®æ•°)
- ã¤ã¾ã‚Š: **768å€‹ã®12ã‚¹ãƒ†ãƒƒãƒ—ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’åŒæ™‚å®Ÿè¡Œ**

### MATWMã§ã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ
```python
# train_agent() å†…
sequences = replay_buffer.sample(768, 1)  # 768å€‹ã®ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹
for seq in sequences:
    z_0 = encode(seq[0])
    for t in range(12):  # å„ã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ã‹ã‚‰12ã‚¹ãƒ†ãƒƒãƒ—æƒ³åƒ
        z_t+1 = predict_next(z_t, action_t)
# â†’ åˆè¨ˆ 768Ã—12 = 9,216ã‚¹ãƒ†ãƒƒãƒ—ã®æƒ³åƒãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
```

---

## ğŸ¯ ä½¿ã„æ–¹

### Notebookã§ã®å®Ÿè¡Œé †åº

```python
# 1. GPUç’°å¢ƒç¢ºèª
gpu_info = print_gpu_info()

# 2. ãƒ¢ãƒ‡ãƒ«ä½œæˆ
shared_wm, shared_wm_opt = MATWMAgent.create_shared_world_model(config, device)
dummy_agent = MATWMAgent(config, 'adversary_0', 0, device, shared_wm)

# 3. é‡ã¿åˆæœŸåŒ–
initialize_matwm_weights(shared_wm, dummy_agent.actor, dummy_agent.critic)

# 4. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç¢ºèª
inspect_matwm_architecture(shared_wm, dummy_agent.actor, dummy_agent.critic, config, device)

# 5. è¨“ç·´å®Ÿè¡Œ
agents, episode_rewards, training_metrics = train_matwm(config)

# 6. å¯è¦–åŒ–
plot_training_progress(episode_rewards, training_metrics)

# 7. ä¿å­˜
save_full_checkpoint(agents, shared_wm, shared_wm_opt, episode_rewards, training_metrics, global_step, 'checkpoint.pt')
```

---

## ğŸ“‚ æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«

### `matwm_utils.py`
å…¨ã¦ã®è¿½åŠ æ©Ÿèƒ½ã‚’å«ã‚€ç·åˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼š
- Weight initialization
- Model saving/loading
- Advanced visualization
- Architecture inspection
- GPU environment info

---

## âœ¨ å®Ÿè£…å®Œäº†

5ã¤ã®æ©Ÿèƒ½å…¨ã¦ãŒå®Ÿè£…ã•ã‚Œã€Notebookã«çµ±åˆã•ã‚Œã¾ã—ãŸï¼

- âœ… é‡ã¿ã®åˆæœŸåŒ–
- âœ… ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿
- âœ… çµŒéã®å¯è¦–åŒ–ï¼ˆ9ãƒ‘ãƒãƒ«è©³ç´°ç‰ˆï¼‰
- âœ… ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç¢ºèªï¼ˆ4é …ç›®å…¨ã¦ï¼‰
- âœ… GPUç’°å¢ƒæŠŠæ¡ï¼ˆå…¨GPUè¡¨ç¤ºãƒ»ç¨®é¡åˆ¤å®šï¼‰

**ã“ã‚Œã§MATWMå®Ÿè£…ã¯å®Œå…¨ã«è«–æ–‡æº–æ‹  + å®Ÿç”¨çš„ãªæ©Ÿèƒ½ã‚’å‚™ãˆã¦ã„ã¾ã™ï¼** ğŸ‰
