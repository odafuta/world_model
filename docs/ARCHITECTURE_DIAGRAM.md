# MATWM ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³è§£

ã“ã®æ–‡æ›¸ã§ã¯ã€å®Ÿè£…ã—ãŸMATWMã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¦–è¦šçš„ã«èª¬æ˜ã—ã¾ã™ã€‚

---

## ğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“å›³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MATWM System                              â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  Environment â”‚         â”‚  Environment â”‚                      â”‚
â”‚  â”‚  (Simple Tag)â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  (Simple Tag)â”‚                      â”‚
â”‚  â”‚              â”‚         â”‚              â”‚                      â”‚
â”‚  â”‚ 3 Adversariesâ”‚         â”‚  1 Good Agentâ”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚         â”‚                        â”‚                              â”‚
â”‚         â–¼                        â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚ MATWM Agent  â”‚         â”‚ MATWM Agent  â”‚                      â”‚
â”‚  â”‚ (Adversary)  â”‚         â”‚ (Good Agent) â”‚                      â”‚
â”‚  â”‚              â”‚         â”‚              â”‚                      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚
â”‚  â”‚  â”‚World   â”‚  â”‚         â”‚  â”‚World   â”‚  â”‚                      â”‚
â”‚  â”‚  â”‚Model   â”‚  â”‚         â”‚  â”‚Model   â”‚  â”‚                      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚
â”‚  â”‚  â”‚Actor   â”‚  â”‚         â”‚  â”‚Actor   â”‚  â”‚                      â”‚
â”‚  â”‚  â”‚Critic  â”‚  â”‚         â”‚  â”‚Critic  â”‚  â”‚                      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚
â”‚  â”‚  â”‚Replay  â”‚  â”‚         â”‚  â”‚Replay  â”‚  â”‚                      â”‚
â”‚  â”‚  â”‚Buffer  â”‚  â”‚         â”‚  â”‚Buffer  â”‚  â”‚                      â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  World Model è©³ç´°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      World Model                             â”‚
â”‚                                                               â”‚
â”‚  Observation (16-dim)                                         â”‚
â”‚         â”‚                                                     â”‚
â”‚         â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚   Encoder   â”‚  MLP: 16 â†’ 512 â†’ 512 â†’ 1024                 â”‚
â”‚  â”‚             â”‚  Output: 32Ã—32 categorical logits           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚         â”‚                                                     â”‚
â”‚         â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚Gumbel-Softmaxâ”‚  Sample: z ~ Cat(logits)                   â”‚
â”‚  â”‚  (hard=True) â”‚  Shape: (batch, 32, 32) one-hot           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚         â”‚                                                     â”‚
â”‚         â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚   Latent    â”‚  z: (batch, 32, 32) one-hot                â”‚
â”‚  â”‚   State     â”‚  Discrete latent representation             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚         â”‚                                                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚    â”‚         â”‚          â”‚          â”‚          â”‚              â”‚
â”‚    â–¼         â–¼          â–¼          â–¼          â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚Dec â”‚  â”‚Dyn   â”‚  â”‚Rewardâ”‚  â”‚Cont  â”‚  â”‚Team  â”‚             â”‚
â”‚  â”‚oderâ”‚  â”‚amics â”‚  â”‚Pred  â”‚  â”‚Pred  â”‚  â”‚Predâ˜… â”‚             â”‚
â”‚  â””â”€â”¬â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”˜             â”‚
â”‚    â”‚         â”‚         â”‚         â”‚         â”‚                â”‚
â”‚    â–¼         â–¼         â–¼         â–¼         â–¼                â”‚
â”‚  Obs'     z_next    reward   continue  teammate_actions      â”‚
â”‚ (recon)   (pred)    (pred)    (pred)    (pred) â˜…            â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ­ Teammate Predictor è©³ç´° â˜…

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Teammate Predictor (Social WM Core)             â”‚
â”‚                                                               â”‚
â”‚  Focal Agent's Latent State: z                               â”‚
â”‚         â”‚                                                     â”‚
â”‚         â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚   Flatten   â”‚  z_flat: (batch, 1024)                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚         â”‚                                                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚    â”‚         â”‚          â”‚          â”‚                         â”‚
â”‚    â–¼         â–¼          â–¼          â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”                              â”‚
â”‚  â”‚MLP â”‚  â”‚MLP â”‚  â”‚MLP â”‚  ...                                 â”‚
â”‚  â”‚ 1  â”‚  â”‚ 2  â”‚  â”‚ 3  â”‚  (num_agents - 1)                    â”‚
â”‚  â””â”€â”¬â”€â”€â”˜  â””â”€â”¬â”€â”€â”˜  â””â”€â”¬â”€â”€â”˜                                      â”‚
â”‚    â”‚       â”‚       â”‚                                         â”‚
â”‚    â–¼       â–¼       â–¼                                         â”‚
â”‚  Agent1  Agent2  Agent3  Action Logits (5-dim each)          â”‚
â”‚  Action  Action  Action                                      â”‚
â”‚                                                               â”‚
â”‚  Each MLP: 1024 â†’ 256 â†’ 256 â†’ 5                              â”‚
â”‚                                                               â”‚
â”‚  â˜… KEY INSIGHT â˜…                                             â”‚
â”‚  From focal agent's state, predict what other agents will do â”‚
â”‚  â†’ Enables coordination (adversaries)                        â”‚
â”‚  â†’ Enables competition (good agent predicts adversaries)     â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Dynamics Model è©³ç´°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Dynamics Model                            â”‚
â”‚                                                               â”‚
â”‚  Sequence: z_0, z_1, ..., z_T                                â”‚
â”‚  Actions:  a_0, a_1, ..., a_T (scaled by agent_idx)          â”‚
â”‚                                                               â”‚
â”‚         z_t              a_t                                 â”‚
â”‚          â”‚               â”‚                                   â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                  â–¼                                           â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚          â”‚Action Mixer â”‚  Embed action + concat with z       â”‚
â”‚          â”‚             â”‚  Output: e_t (512-dim)             â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                 â”‚                                            â”‚
â”‚                 â–¼                                            â”‚
â”‚   e_0, e_1, ..., e_T  (sequence of mixed embeddings)         â”‚
â”‚                 â”‚                                            â”‚
â”‚                 â–¼                                            â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚          â”‚ Transformer â”‚  4 layers, 8 heads                 â”‚
â”‚          â”‚   Encoder   â”‚  512-dim hidden                    â”‚
â”‚          â”‚             â”‚  GELU activation                    â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                 â”‚                                            â”‚
â”‚                 â–¼                                            â”‚
â”‚   h_0, h_1, ..., h_T  (hidden states)                        â”‚
â”‚                 â”‚                                            â”‚
â”‚                 â–¼                                            â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚          â”‚ Dynamics    â”‚  MLP: 512 â†’ 512 â†’ 1024             â”‚
â”‚          â”‚   Head      â”‚  Reshape: (batch, T, 32, 32)       â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                 â”‚                                            â”‚
â”‚                 â–¼                                            â”‚
â”‚          z_next_logits  (predicted next state distribution)  â”‚
â”‚                 â”‚                                            â”‚
â”‚                 â–¼                                            â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚          â”‚Gumbel-Softmaxâ”‚  Sample z_next                     â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Actor-Critic è©³ç´°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Actor-Critic                             â”‚
â”‚                                                               â”‚
â”‚  Latent State: z (from World Model)                          â”‚
â”‚         â”‚                                                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”                                               â”‚
â”‚    â”‚         â”‚                                               â”‚
â”‚    â–¼         â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚Actorâ”‚   â”‚Criticâ”‚                                          â”‚
â”‚  â””â”€â”¬â”€â”€â”˜   â””â”€â”¬â”€â”€â”˜                                             â”‚
â”‚    â”‚        â”‚                                                â”‚
â”‚    â–¼        â–¼                                                â”‚
â”‚  Action   Value                                              â”‚
â”‚  Logits   V(z)                                               â”‚
â”‚    â”‚        â”‚                                                â”‚
â”‚    â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚    â–¼                         â–¼                               â”‚
â”‚  Sample                   Compute                            â”‚
â”‚  Action                   Advantage                          â”‚
â”‚    â”‚                         â”‚                               â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚              â–¼                                               â”‚
â”‚       Update with                                            â”‚
â”‚    Policy Gradient                                           â”‚
â”‚                                                               â”‚
â”‚  Actor: 1024 â†’ 256 â†’ 256 â†’ 5                                â”‚
â”‚  Critic: 1024 â†’ 256 â†’ 256 â†’ 1                               â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Training Loop ãƒ•ãƒ­ãƒ¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Training Loop                            â”‚
â”‚                                                               â”‚
â”‚  1. INTERACT WITH ENVIRONMENT                                 â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚     â”‚ obs_t â†’ Actor â†’ action_t             â”‚                 â”‚
â”‚     â”‚ env.step(action_t) â†’ obs_t+1, r_t    â”‚                 â”‚
â”‚     â”‚ Store in Replay Buffer               â”‚                 â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  2. TRAIN WORLD MODEL (Prioritized Replay)                   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚     â”‚ Sample sequences (recent prioritized)â”‚                 â”‚
â”‚     â”‚                                       â”‚                 â”‚
â”‚     â”‚ Losses:                               â”‚                 â”‚
â”‚     â”‚  - Reconstruction: MSE(obs, recon)   â”‚                 â”‚
â”‚     â”‚  - Dynamics: CE(z_next, z_target)    â”‚                 â”‚
â”‚     â”‚  - Reward: CE(r_pred, r_target)      â”‚                 â”‚
â”‚     â”‚  - Continuation: BCE(cont, target)   â”‚                 â”‚
â”‚     â”‚  - Teammate: CE(a_pred, a_actual) â˜…  â”‚                 â”‚
â”‚     â”‚  - KL: KL(z_post || z_prior)         â”‚                 â”‚
â”‚     â”‚                                       â”‚                 â”‚
â”‚     â”‚ Update: World Model parameters        â”‚                 â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  3. TRAIN AGENT (Imagination-based)                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚     â”‚ Sample start states (uniform)        â”‚                 â”‚
â”‚     â”‚                                       â”‚                 â”‚
â”‚     â”‚ Imagination Rollout (H=15):          â”‚                 â”‚
â”‚     â”‚   z_0 â†’ Actor â†’ a_0                  â”‚                 â”‚
â”‚     â”‚   z_0, a_0 â†’ WM â†’ z_1, r_1, cont_1   â”‚                 â”‚
â”‚     â”‚   z_1 â†’ Actor â†’ a_1                  â”‚                 â”‚
â”‚     â”‚   ...                                 â”‚                 â”‚
â”‚     â”‚   z_H â†’ Value                        â”‚                 â”‚
â”‚     â”‚                                       â”‚                 â”‚
â”‚     â”‚ Compute GAE advantages               â”‚                 â”‚
â”‚     â”‚                                       â”‚                 â”‚
â”‚     â”‚ Update:                               â”‚                 â”‚
â”‚     â”‚  - Actor: Policy gradient            â”‚                 â”‚
â”‚     â”‚  - Critic: TD error                  â”‚                 â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚  4. REPEAT until total_steps reached                         â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Data Flow                              â”‚
â”‚                                                               â”‚
â”‚  Environment Observation                                      â”‚
â”‚         â”‚                                                     â”‚
â”‚         â”‚ obs_t (14 or 16 dim)                               â”‚
â”‚         â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚   Encoder   â”‚                                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚         â”‚ z_t (32Ã—32 one-hot)                                â”‚
â”‚         â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚   Actor     â”‚          â”‚  Dynamics   â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚         â”‚ action_t              â”‚ z_t+1                      â”‚
â”‚         â–¼                       â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ Environment â”‚          â”‚   Decoder   â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚         â”‚ obs_t+1, r_t          â”‚ obs_recon                  â”‚
â”‚         â–¼                       â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚Replay Bufferâ”‚          â”‚   Losses    â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚         â”‚                                                     â”‚
â”‚         â–¼                                                     â”‚
â”‚  Training (World Model & Agent)                               â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤ Multi-Agent Interaction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Multi-Agent Interaction (Simple Tag)            â”‚
â”‚                                                               â”‚
â”‚  Time t:                                                      â”‚
â”‚                                                               â”‚
â”‚  Agent 0 (Adversary):                                         â”‚
â”‚    obs_0 â†’ WM_0 â†’ z_0                                        â”‚
â”‚    z_0 â†’ Actor_0 â†’ a_0                                       â”‚
â”‚    z_0 â†’ TeammatePredictor_0 â†’ [a_1_pred, a_2_pred, a_3_pred]â”‚
â”‚                                                               â”‚
â”‚  Agent 1 (Adversary):                                         â”‚
â”‚    obs_1 â†’ WM_1 â†’ z_1                                        â”‚
â”‚    z_1 â†’ Actor_1 â†’ a_1                                       â”‚
â”‚    z_1 â†’ TeammatePredictor_1 â†’ [a_0_pred, a_2_pred, a_3_pred]â”‚
â”‚                                                               â”‚
â”‚  Agent 2 (Adversary):                                         â”‚
â”‚    obs_2 â†’ WM_2 â†’ z_2                                        â”‚
â”‚    z_2 â†’ Actor_2 â†’ a_2                                       â”‚
â”‚    z_2 â†’ TeammatePredictor_2 â†’ [a_0_pred, a_1_pred, a_3_pred]â”‚
â”‚                                                               â”‚
â”‚  Agent 3 (Good):                                              â”‚
â”‚    obs_3 â†’ WM_3 â†’ z_3                                        â”‚
â”‚    z_3 â†’ Actor_3 â†’ a_3                                       â”‚
â”‚    z_3 â†’ TeammatePredictor_3 â†’ [a_0_pred, a_1_pred, a_2_pred]â”‚
â”‚                                                               â”‚
â”‚  Environment:                                                 â”‚
â”‚    step([a_0, a_1, a_2, a_3])                                â”‚
â”‚    â†’ [obs'_0, obs'_1, obs'_2, obs'_3]                        â”‚
â”‚    â†’ [r_0, r_1, r_2, r_3]                                    â”‚
â”‚                                                               â”‚
â”‚  Learning:                                                    â”‚
â”‚    Each agent learns:                                        â”‚
â”‚    - World Model: obs â†’ z â†’ obs', r, cont                    â”‚
â”‚    - Teammate Pred: z â†’ other_agents_actions â˜…               â”‚
â”‚    - Actor: z â†’ action                                       â”‚
â”‚    - Critic: z â†’ value                                       â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Loss Functions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Loss Functions                           â”‚
â”‚                                                               â”‚
â”‚  WORLD MODEL LOSSES:                                          â”‚
â”‚                                                               â”‚
â”‚  1. Reconstruction Loss                                       â”‚
â”‚     L_recon = MSE(Decoder(z), obs)                           â”‚
â”‚     â†“                                                        â”‚
â”‚     Ensures latent captures observation info                 â”‚
â”‚                                                               â”‚
â”‚  2. Dynamics Loss                                             â”‚
â”‚     L_dyn = CrossEntropy(z_next_pred, z_next_target)         â”‚
â”‚     â†“                                                        â”‚
â”‚     Ensures accurate next-state prediction                   â”‚
â”‚                                                               â”‚
â”‚  3. Reward Loss (Two-hot Symlog)                             â”‚
â”‚     reward_symlog = sign(r) * log(1 + |r|)                   â”‚
â”‚     reward_2hot = TwoHotEncode(reward_symlog)                â”‚
â”‚     L_reward = CrossEntropy(r_pred, reward_2hot)             â”‚
â”‚     â†“                                                        â”‚
â”‚     Robust reward prediction                                 â”‚
â”‚                                                               â”‚
â”‚  4. Continuation Loss                                         â”‚
â”‚     L_cont = BCE(cont_pred, 1 - done)                        â”‚
â”‚     â†“                                                        â”‚
â”‚     Episode termination prediction                           â”‚
â”‚                                                               â”‚
â”‚  5. Teammate Loss â˜… (Social World Model)                     â”‚
â”‚     L_team = mean([CE(a_pred_i, a_actual_i)                  â”‚
â”‚                   for i in other_agents])                    â”‚
â”‚     â†“                                                        â”‚
â”‚     Learn to predict other agents' actions                   â”‚
â”‚                                                               â”‚
â”‚  6. KL Divergence (with Free Nats)                           â”‚
â”‚     L_kl = max(KL(posterior || prior), free_nats)            â”‚
â”‚     â†“                                                        â”‚
â”‚     Regularization with minimum complexity                   â”‚
â”‚                                                               â”‚
â”‚  Total: L_WM = L_recon + L_dyn + L_reward +                  â”‚
â”‚                L_cont + 0.5*L_team + L_kl                    â”‚
â”‚                                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚                                                               â”‚
â”‚  AGENT LOSSES:                                                â”‚
â”‚                                                               â”‚
â”‚  7. Actor Loss (Policy Gradient)                             â”‚
â”‚     L_actor = -mean(log Ï€(a|z) * advantage)                  â”‚
â”‚     â†“                                                        â”‚
â”‚     Maximize expected returns                                â”‚
â”‚                                                               â”‚
â”‚  8. Critic Loss (TD Error)                                    â”‚
â”‚     L_critic = MSE(V(z), returns)                            â”‚
â”‚     where returns = advantages + values                      â”‚
â”‚     â†“                                                        â”‚
â”‚     Accurate value estimation                                â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Key Implementation Details

### Gumbel-Softmax Trick

```python
# Forward pass: hard one-hot
# Backward pass: soft probabilities
z = F.gumbel_softmax(logits, tau=1.0, hard=True)
```

### Action Scaling

```python
# Agent 0: 0, 1, 2, 3, 4
# Agent 1: 5, 6, 7, 8, 9
# Agent 2: 10, 11, 12, 13, 14
# Agent 3: 15, 16, 17, 18, 19
scaled_action = action + agent_idx * action_dim
```

### Prioritized Replay

```python
# New experience: priority = 1.0
# Every step: priority *= 0.995
# Sampling: p âˆ priority
```

### Two-Hot Encoding

```python
# Continuous value â†’ 2 adjacent bins
# Smooth interpolation between bins
# Better gradient flow
```

---

ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³è§£ã«ã‚ˆã‚Šã€MATWMã®æ§‹é€ ã¨å‹•ä½œãŒç†è§£ã§ããŸã¨æ€ã„ã¾ã™ï¼

**Key Point**: Teammate Predictor ãŒç¤¾ä¼šçš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®ã‚³ã‚¢ã§ã‚ã‚Šã€
ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•äºˆæ¸¬ã«ã‚ˆã‚Šå”èª¿ãƒ»ç«¶äº‰è¡Œå‹•ã‚’æ”¹å–„ã—ã¾ã™ã€‚


