# MATWM Implementation Summary

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

**ç¤¾ä¼šçš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ« (Social World Model)** ã‚’PettingZooã®`simple_tag`ç’°å¢ƒã«å®Ÿè£…ã—ã¾ã—ãŸã€‚

- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: MATWM (Multi-Agent Transformer World Model)
- **è«–æ–‡**: Deihim et al. (2025), "Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning", arXiv:2506.18537
- **ç’°å¢ƒ**: Simple Tag (predator-prey, 3 adversaries vs 1 good agent)
- **ç›®æ¨™**: ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡çš„ãªãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ (100K stepsä»¥å†…)

---

## å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«

### 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
æœ€çµ‚èª²é¡Œ/
â”œâ”€â”€ PROJECT_STRUCTURE.md                          # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã®è©³ç´°èª¬æ˜
â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md                     # æœ¬ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ README.md                                     # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦ãƒ»é¸å®šç†ç”±
â”œâ”€â”€ simple_tag.md                                 # ç’°å¢ƒä»•æ§˜
â”‚
â”œâ”€â”€ matwm_implementation.py                       # World Modelã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ â˜…
â”‚   â”œâ”€â”€ MATWMConfig                               # è¨­å®šã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ PrioritizedReplayBuffer                   # å„ªå…ˆåº¦ä»˜ããƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡
â”‚   â”œâ”€â”€ Encoder/Decoder                           # Categorical VAE
â”‚   â”œâ”€â”€ DynamicsModel                             # Transformer dynamics
â”‚   â”œâ”€â”€ RewardPredictor                           # å ±é…¬äºˆæ¸¬
â”‚   â”œâ”€â”€ ContinuationPredictor                     # ç¶™ç¶šäºˆæ¸¬
â”‚   â”œâ”€â”€ TeammatePredictor â˜…                       # ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•äºˆæ¸¬
â”‚   â”œâ”€â”€ Actor/Critic                              # Actor-Critic networks
â”‚   â””â”€â”€ Utility functions                         # symlog, two-hot, etc.
â”‚
â”œâ”€â”€ matwm_agent.py                                # å®Œå…¨ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£… â˜…
â”‚   â””â”€â”€ MATWMAgent                                # è¨“ç·´ãƒ«ãƒ¼ãƒ—å«ã‚€å®Œå…¨å®Ÿè£…
â”‚
â”œâ”€â”€ 2026_MATWM_simple_tag_Implementation.ipynb   # ãƒ¡ã‚¤ãƒ³Notebook â˜…
â”‚   â”œâ”€â”€ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
â”‚   â”œâ”€â”€ ç’°å¢ƒç¢ºèª
â”‚   â”œâ”€â”€ è¨“ç·´ãƒ«ãƒ¼ãƒ—
â”‚   â”œâ”€â”€ å¯è¦–åŒ–
â”‚   â””â”€â”€ è©•ä¾¡
â”‚
â”œâ”€â”€ data/                                         # ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ
â”‚   â””â”€â”€ replay_buffers/
â”‚
â””â”€â”€ results/                                      # å­¦ç¿’çµæœä¿å­˜å…ˆ
    â”œâ”€â”€ checkpoints/
    â”œâ”€â”€ training_curves.png
    â””â”€â”€ logs/
```

---

## ã‚³ã‚¢å®Ÿè£…: ç¤¾ä¼šçš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«

### Teammate Predictor â˜…

**æœ€ã‚‚é‡è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**: ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã‚’äºˆæ¸¬

```python
class TeammatePredictor(nn.Module):
    """
    Predict other agents' actions from focal agent's latent state
    
    â˜… This is the CORE component for social world modeling â˜…
    
    It enables the focal agent to anticipate behaviors of other agents,
    which is crucial for coordination and competition.
    """
    
    def __init__(self, latent_dim=32, num_classes=32, action_dim=5, 
                 num_agents=4, hidden_dim=256):
        super().__init__()
        self.num_agents = num_agents
        self.action_dim = action_dim
        
        # Separate predictor for each other agent
        self.predictors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(latent_dim * num_classes, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, action_dim)
            )
            for _ in range(num_agents - 1)
        ])
    
    def forward(self, z, focal_agent_idx):
        """Predict actions of all other agents"""
        z_flat = z.reshape(*z.shape[:-2], -1)
        
        teammate_logits = {}
        predictor_idx = 0
        for agent_idx in range(self.num_agents):
            if agent_idx != focal_agent_idx:
                logits = self.predictors[predictor_idx](z_flat)
                teammate_logits[agent_idx] = logits
                predictor_idx += 1
        
        return teammate_logits
```

**åŠ¹æœ**:
- éå®šå¸¸æ€§ã®è»½æ¸›: ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ–¹ç­–å¤‰åŒ–ã«å¯¾å¿œ
- å”èª¿è¡Œå‹•ã®ä¿ƒé€²: adversariesåŒå£«ã®é€£æº
- ç«¶äº‰è¡Œå‹•ã®æ”¹å–„: good agentãŒadversariesã®å‹•ãã‚’äºˆæ¸¬ã—ã¦é€ƒã’ã‚‹

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

### World Model

#### 1. Encoder/Decoder (Categorical VAE)

- **å…¥åŠ›**: ãƒ™ã‚¯ãƒˆãƒ«è¦³æ¸¬ (14æ¬¡å…ƒ or 16æ¬¡å…ƒ)
- **æ½œåœ¨ç©ºé–“**: 32 categorical variables Ã— 32 classes = 1024æ¬¡å…ƒé›¢æ•£ç©ºé–“
- **ç‰¹å¾´**: Gumbel-Softmax ã«ã‚ˆã‚‹å¾®åˆ†å¯èƒ½ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

#### 2. Dynamics Model (Transformer)

- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: 4å±¤, 8ãƒ˜ãƒƒãƒ‰, 512æ¬¡å…ƒ
- **å…¥åŠ›**: æ½œåœ¨çŠ¶æ…‹ç³»åˆ— + è¡Œå‹•ç³»åˆ— (action scaled by agent ID)
- **å‡ºåŠ›**: æ¬¡æ™‚åˆ»ã®æ½œåœ¨çŠ¶æ…‹åˆ†å¸ƒ

#### 3. Reward Predictor

- **ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: Two-hot symlog (Dreamer V3)
- **åˆ©ç‚¹**: æ¥µç«¯ãªå ±é…¬å€¤ã«ã‚‚ãƒ­ãƒã‚¹ãƒˆ

#### 4. Continuation Predictor

- **å‡ºåŠ›**: Bernoulliåˆ†å¸ƒ (ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç¶™ç¶šç¢ºç‡)

### Agent (Actor-Critic)

#### Actor

- **æ–¹ç­–**: Categoricalåˆ†å¸ƒ
- **å­¦ç¿’**: Policy gradient with imagined advantages

#### Critic

- **ä¾¡å€¤é–¢æ•°**: V(z) - æ½œåœ¨çŠ¶æ…‹ã‹ã‚‰ä¾¡å€¤ã‚’æ¨å®š
- **ç‰¹å¾´**: Semi-centralized - ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æƒ³åƒè¡Œå‹•ã‚’è€ƒæ…®

---

## è¨“ç·´æˆ¦ç•¥

### 1. Prioritized Replay Buffer

- **å„ªå…ˆåº¦**: æœ€è¿‘ã®çµŒé¨“ã»ã©é«˜ã„é‡ã¿ (exponential decay)
- **ç†ç”±**: ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ–¹ç­–å¤‰åŒ–ã«è¿½å¾“

```python
# Priority decay per step
self.priorities = deque([p * 0.995 for p in self.priorities])
```

### 2. Action Scaling

- **Agent 0**: actions 0-4
- **Agent 1**: actions 5-9
- **Agent 2**: actions 10-14
- **Agent 3**: actions 15-19

ã“ã‚Œã«ã‚ˆã‚Šã€ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã‹ã‚’è­˜åˆ¥å¯èƒ½ã€‚

### 3. Imagination-based Training

- **å®Ÿç’°å¢ƒ**: 1ã‚¹ãƒ†ãƒƒãƒ— â†’ **æƒ³åƒ**: 15ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã®å­¦ç¿’
- **ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡**: 15å€ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ

---

## è¨“ç·´ãƒ•ãƒ­ãƒ¼

### Phase 1: Warmup (1000 steps)

- ãƒ©ãƒ³ãƒ€ãƒ æ–¹ç­–ã§Replay Bufferã‚’åŸ‹ã‚ã‚‹
- å­¦ç¿’ã¯è¡Œã‚ãªã„

### Phase 2: Joint Training (æ®‹ã‚Šã®steps)

å„ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«:

1. **ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨**
   - å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒActorã§è¡Œå‹•é¸æŠ
   - ç’°å¢ƒã‹ã‚‰reward, next_obs, doneã‚’å–å¾—
   - Replay Bufferã«ä¿å­˜

2. **World Modelè¨“ç·´** (å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)
   - Prioritized Replayã‹ã‚‰Sequence sampling
   - 6ã¤ã®æå¤±é–¢æ•°ã‚’æœ€å°åŒ–:
     - Reconstruction loss
     - Dynamics loss
     - Reward loss
     - Continuation loss
     - **Teammate prediction loss** â˜…
     - KL divergence (with free nats)

3. **Agentè¨“ç·´** (å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)
   - Random sampling (uniform)
   - æƒ³åƒãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ (horizon=15)
   - Actor loss (policy gradient)
   - Critic loss (TD error)

### Phase 3: Evaluation

- Deterministic policy
- è¤‡æ•°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§å¹³å‡å ±é…¬ã‚’è¨ˆç®—

---

## æå¤±é–¢æ•°

### World Model Total Loss

```
L_total = L_recon + L_dynamics + L_reward + L_cont + 0.5 * L_teammate + L_kl
```

#### 1. Reconstruction Loss

```python
L_recon = MSE(decoder(z), obs)
```

#### 2. Dynamics Loss

```python
L_dynamics = CrossEntropy(z_next_pred, z_next_target)
```

#### 3. Reward Loss (Two-hot Symlog)

```python
reward_symlog = symlog(reward)
reward_target = two_hot_encode(reward_symlog)
L_reward = CrossEntropy(reward_pred, reward_target)
```

#### 4. Continuation Loss

```python
L_cont = BCE(continuation_pred, 1 - done)
```

#### 5. Teammate Prediction Loss â˜…

```python
L_teammate = mean([
    CrossEntropy(teammate_pred[agent_i], actual_action[agent_i])
    for agent_i in other_agents
])
```

#### 6. KL Divergence (with Free Nats)

```python
L_kl = max(KL(z_posterior || z_prior), free_nats)
```

### Agent Losses

#### Actor Loss

```python
L_actor = -mean(log_prob(action) * advantage)
```

#### Critic Loss

```python
L_critic = MSE(V(z), returns)
```

---

## ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

### ãƒ¢ãƒ‡ãƒ«

- Latent dim: 32 Ã— 32 = 1024
- Hidden dim: 512
- Transformer layers: 4
- Attention heads: 8

### è¨“ç·´

- Batch size: 16
- Sequence length: 64
- Imagination horizon: 15
- Learning rate: 3e-4
- Î³ (discount): 0.99
- Î» (GAE): 0.95

### Buffer

- Capacity: 100,000
- Priority decay: 0.995

### å®Ÿè¡Œ

- Total steps: 100,000 (ãƒ•ãƒ«è¨“ç·´)
- Warmup: 1,000
- Save interval: 5,000

---

## ä½¿ç”¨æ–¹æ³•

### 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
pip install torch numpy matplotlib tqdm
pip install pettingzoo[mpe] supersuit
```

### 2. è¨“ç·´ã®å®Ÿè¡Œ

#### Notebookå®Ÿè¡Œ

```bash
jupyter notebook 2026_MATWM_simple_tag_Implementation.ipynb
```

#### ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ

```python
from matwm_implementation import MATWMConfig
from matwm_agent import MATWMAgent
from pettingzoo.mpe import simple_tag_v3

# Configuration
config = MATWMConfig(total_steps=100000)

# Create environment
env = simple_tag_v3.parallel_env(...)

# Create agents
agents = {name: MATWMAgent(config, name, idx, device) 
          for idx, name in enumerate(env.agents)}

# Train (see notebook for full loop)
```

### 3. è©•ä¾¡

```python
def evaluate_agents(agents, num_episodes=20):
    # ... (see notebook)
    pass

eval_rewards = evaluate_agents(agents)
```

---

## æœŸå¾…ã•ã‚Œã‚‹çµæœ

### ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡

- **ç›®æ¨™**: 50K-100K steps ã§åæŸ
- **æ¯”è¼ƒ**: å¾“æ¥ã®model-freeæ‰‹æ³•ã¯1M+ stepså¿…è¦

### æ€§èƒ½

#### Adversaries (predators)

- åˆæœŸ: ãƒ©ãƒ³ãƒ€ãƒ ã«å‹•ã
- å­¦ç¿’å¾Œ: å”èª¿ã—ã¦good agentã‚’è¿½è·¡ãƒ»åŒ…å›²
- Teammate Predictorã«ã‚ˆã‚Šä»–ã®adversariesã®å‹•ãã‚’äºˆæ¸¬

#### Good Agent (prey)

- åˆæœŸ: é€ƒã’ã‚‰ã‚Œãªã„
- å­¦ç¿’å¾Œ: adversariesã®å‹•ãã‚’äºˆæ¸¬ã—ã¦åŠ¹ç‡çš„ã«é€ƒã’ã‚‹
- éšœå®³ç‰©ã‚’åˆ©ç”¨ã—ãŸæˆ¦ç•¥

### å­¦ç¿’æ›²ç·š

- **Adversaries**: å ±é…¬ãŒå¾ã€…ã«ä¸Šæ˜‡ (0 â†’ +10ä»˜è¿‘)
- **Good Agent**: å ±é…¬ãŒæ”¹å–„ (-10 â†’ -5ä»˜è¿‘)
- **Teammate Loss**: å¾ã€…ã«æ¸›å°‘ â†’ ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆäºˆæ¸¬ã®ç²¾åº¦å‘ä¸Š

---

## å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ

### 1. Categorical VAE

- Gumbel-Softmax trick for differentiability
- One-hot encoding in forward pass
- Soft probabilities in backward pass

### 2. Action Scaling

```python
scaled_action = action + agent_idx * action_dim
```

ã“ã‚Œã«ã‚ˆã‚Šå„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®action spaceãŒé‡è¤‡ã—ãªã„

### 3. Prioritized Replay

- World Modelè¨“ç·´: Prioritized (recenté‡è¦–)
- Agentè¨“ç·´: Uniform (diverse experiences)

### 4. Imagination Rollout

- Detach after each step to prevent long gradient chains
- Use world model in eval mode during imagination

### 5. Two-hot Encoding

- é€£ç¶šå€¤ã‚’2ã¤ã®ãƒ“ãƒ³ã«åˆ†æ•£
- ã‚ˆã‚Šsmooth ãªå­¦ç¿’

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å•é¡Œ1: å­¦ç¿’ãŒé€²ã¾ãªã„

**åŸå› **: Replay BufferãŒå°ã•ã™ãã‚‹ / WarmupãŒçŸ­ã„

**è§£æ±ºç­–**:
```python
config.buffer_size = 100000
config.warmup_steps = 2000
```

### å•é¡Œ2: Teammate LossãŒä¸‹ãŒã‚‰ãªã„

**åŸå› **: ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ–¹ç­–ãŒã¾ã ãƒ©ãƒ³ãƒ€ãƒ  / å­¦ç¿’ç‡ãŒé«˜ã™ãã‚‹

**è§£æ±ºç­–**:
- WarmupæœŸé–“ã‚’å»¶ã°ã™
- Teammate weightã‚’èª¿æ•´: `config.teammate_weight = 0.3`

### å•é¡Œ3: ãƒ¡ãƒ¢ãƒªä¸è¶³

**åŸå› **: Sequence lengthãŒé•·ã™ãã‚‹ / Batch sizeãŒå¤§ãã™ãã‚‹

**è§£æ±ºç­–**:
```python
config.sequence_length = 32  # 64 â†’ 32
config.batch_size = 8  # 16 â†’ 8
```

### å•é¡Œ4: å­¦ç¿’ãŒä¸å®‰å®š

**åŸå› **: Learning rateãŒé«˜ã™ãã‚‹ / Gradient explosion

**è§£æ±ºç­–**:
```python
config.learning_rate = 1e-4  # 3e-4 â†’ 1e-4
# Gradient clippingã¯æ—¢ã«å®Ÿè£…æ¸ˆã¿ (max_norm=100)
```

---

## ä»Šå¾Œã®æ‹¡å¼µ

### 1. Î³-Progress Curiosity

Active World Model Learningã®æ‰‹æ³•:

```python
class ProgressCuriosity:
    def compute_intrinsic_reward(self, z_curr, z_pred, z_actual):
        error_new = F.mse_loss(z_pred, z_actual)
        progress = self.error_old - error_new
        self.error_old = error_new
        return torch.clamp(progress, 0, 1)
```

### 2. Theory of Mind

ã‚ˆã‚Šé«˜åº¦ãªç¤¾ä¼šçš„æ¨è«–:

- ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¿¡å¿µãƒ»æ„å›³ã®æ¨å®š
- Recursive reasoning
- Mental state tracking

### 3. Communication

```python
class CommunicationModule(nn.Module):
    def forward(self, z_focal, z_others):
        # Attention-based message passing
        messages = self.attention(z_focal, z_others)
        z_augmented = torch.cat([z_focal, messages], dim=-1)
        return z_augmented
```

### 4. éšå±¤çš„ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°

- High-level: Goal selection
- Low-level: Action execution

### 5. Self-Play

- Population-based training
- Best response dynamics

---

## å‚è€ƒæ–‡çŒ®

1. **MATWM**: Deihim, A., Alonso, E., & Apostolopoulou, D. (2025). *Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning*. arXiv:2506.18537.

2. **Dreamer V3**: Hafner, D., Rusu, A., Veness, J., Duan, J., Hadsell, R., & Wayne, G. (2023). *Mastering Diverse Domains through World Models*.

3. **STORM**: [Single-agent Transformer World Model]

4. **Categorical VAE**: Jang, E., Gu, S., & Poole, B. (2016). *Categorical Reparameterization with Gumbel-Softmax*.

5. **PettingZoo**: Terry, J. K., et al. (2021). *PettingZoo: Gym for Multi-Agent Reinforcement Learning*.

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

This implementation is for educational purposes.

---

## é€£çµ¡å…ˆ

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã¨çŸ¥èƒ½ 2025 æœ€çµ‚èª²é¡Œ

---

**å®Ÿè£…å®Œäº†!** ğŸ‰

ç¤¾ä¼šçš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ã®å®Œå…¨ãªå®Ÿè£…ãŒå®Œäº†ã—ã¾ã—ãŸã€‚


